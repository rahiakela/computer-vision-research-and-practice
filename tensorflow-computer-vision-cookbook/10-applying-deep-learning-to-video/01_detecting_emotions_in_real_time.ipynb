{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-detecting-emotions-in-real-time.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNblTWr4pjTGgo1+yI2VRBy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/tensorflow-computer-vision-cookbook/blob/main/10-applying-deep-learning-to-video/01_detecting_emotions_in_real_time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwjrPlYUUAR0"
      },
      "source": [
        "##Detecting emotions in real time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLpvsvKBWcdq"
      },
      "source": [
        "Computer vision is focused on the understanding of visual data. Of course, that includes videos, which, at their core, are a sequence of images, which means we can leverage most of our knowledge regarding deep learning for image processing to videos and reap great results.\n",
        "\n",
        "In this recipe, we'll start training a convolutional neuronal network to detect emotions in human faces, and then we'll learn how to apply it in a real-time context using our webcam.\n",
        "\n",
        "Then, in the remaining recipes, we'll use very advanced implementations of architectures, hosted in TensorFlow Hub (TFHub), specially tailored to tackle interesting video-related problems such as action recognition, frames generation, and text-to-video retrieval.\n",
        "\n",
        "At its most basic form, a video is just a series of images. By leveraging this seemingly simple or trivial fact, we can adapt what we know about image classification to create very interesting video processing pipelines powered by deep learning.\n",
        "\n",
        "In this recipe, we'll build an algorithm to detect emotions in real time (webcam streaming) or from video files. Pretty interesting, right?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr83uKG6Wr-V"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ_dF9j0Z_l0"
      },
      "source": [
        "Let's download Facial Expression Recognition Challenge from [Kaggle](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI1lHbZHaKpN"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload() # upload kaggle.json file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueCTuNsVaMvJ"
      },
      "source": [
        "%%shell\n",
        "\n",
        "mkdir -p ~/.kaggle\n",
        "mv kaggle.json ~/.kaggle/\n",
        "ls ~/.kaggle\n",
        "chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# download dataset from kaggle\n",
        "kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "\n",
        "# unzip files\n",
        "unzip -qq train.csv.zip\n",
        "unzip -qq test.csv.zip\n",
        "unzip -qq icml_face_data.csv.zip\n",
        "gzip -d fer2013.tar.gz\n",
        "\n",
        "rm -rf train.csv.zip\n",
        "rm -rf test.csv.zip\n",
        "rm -rf icml_face_data.csv.zip\n",
        "rm -rf fer2013.tar.gz\n",
        "rm -rf fer2013.tar.gz\n",
        "\n",
        "# untar file\n",
        "mkdir emotion_recognition\n",
        "tar -xvf fer2013.tar -C emotion_recognition/\n",
        "rm -rf fer2013.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpwaI4gbbCXO"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget -q https://github.com/rahiakela/tensorflow-computer-vision-cookbook/raw/main/10-applying-deep-learning-to-video/videos/emotions.mp4\n",
        "wget -q https://github.com/rahiakela/tensorflow-computer-vision-cookbook/raw/main/10-applying-deep-learning-to-video/videos/emotions2.mp4\n",
        "\n",
        "wget -q https://github.com/rahiakela/tensorflow-computer-vision-cookbook/raw/main/10-applying-deep-learning-to-video/resources/haarcascade_frontalface_default.xml\n",
        "wget -q https://github.com/rahiakela/tensorflow-computer-vision-cookbook/raw/main/10-applying-deep-learning-to-video/models/model-ep061-loss0.795-val_loss0.882.h5\n",
        "\n",
        "mkdir resources\n",
        "mkdir models\n",
        "mkdir test_videos\n",
        "mv haarcascade_frontalface_default.xml resources/\n",
        "mv model-ep061-loss0.795-val_loss0.882.h5 models/\n",
        "mv *.mp4.txt test_videos/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_bi0od5WtZO"
      },
      "source": [
        "import csv\n",
        "import glob\n",
        "import pathlib\n",
        "import cv2\n",
        "import imutils\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import *\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVhiU88Ra3Fa"
      },
      "source": [
        "## Define some helper function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUM7C51Ga61o"
      },
      "source": [
        "Let's define a list of all possible emotions in our dataset, along with a color associated with each one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFoXzw30NAO4"
      },
      "source": [
        "EMOTIONS = [\"angry\", \"scared\", \"happy\". \"sad\", \"surprised\", \"neutral\"]\n",
        "COLORS = [\n",
        "  \"angry\": (0, 0, 255), \n",
        "  \"scared\": (0, 128, 255), \n",
        "  \"happy\": (0, 255, 255), \n",
        "  \"sad\": (255, 0, 0), \n",
        "  \"surprised\": (178, 255, 102), \n",
        "  \"neutral\": (160, 160, 160)         \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR8ijnofPEPm"
      },
      "source": [
        "Let's define a method to build the emotion classifier architecture. It receives the input shape and the number of classes in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H50dKDGTOvlu"
      },
      "source": [
        "def build_network(input_shape, classes):\n",
        "\n",
        "  input = Input(shape=input_shape)\n",
        "  \"\"\"\n",
        "  Each block in the network is comprised of two ELU activated, batch-normalized\n",
        "  convolutions, followed by a max pooling layer, and ending with a dropout layer.\n",
        "  \"\"\"\n",
        "  x = Conv2D(filters=32,\n",
        "             kernel_size=(3, 3),\n",
        "             padding=\"same\",\n",
        "             kernel_initializer=\"he_normal\")(input)\n",
        "  x = ELU()(x)\n",
        "  x = BatchNormalization(axis=-1)(x)\n",
        "  x = Conv2D(filters=32,\n",
        "             kernel_size=(3, 3),\n",
        "             kernel_initializer=\"he_normal\",\n",
        "             padding=\"same\")(x)\n",
        "  x = ELU()(x)\n",
        "  x = BatchNormalization(axis=-1)(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = Dropout(rate=0.25)(x)\n",
        "\n",
        "  x = Conv2D(filters=64,\n",
        "             kernel_size=(3, 3),\n",
        "             kernel_initializer=\"he_normal\",\n",
        "             padding=\"same\")(x)\n",
        "  x = ELU()(x)\n",
        "  x = BatchNormalization(axis=-1)(x)\n",
        "  x = Conv2D(filters=64,\n",
        "             kernel_size=(3, 3),\n",
        "             kernel_initializer=\"he_normal\",\n",
        "             padding=\"same\")(x)\n",
        "  x = ELU()(x)\n",
        "  x = BatchNormalization(axis=-1)(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = Dropout(rate=0.25)(x)\n",
        "\n",
        "  x = Conv2D(filters=128,\n",
        "             kernel_size=(3, 3),\n",
        "             kernel_initializer=\"he_normal\",\n",
        "             padding=\"same\")(x)\n",
        "  x = ELU()(x)\n",
        "  x = BatchNormalization(axis=-1)(x)\n",
        "  x = Conv2D(filters=128,\n",
        "             kernel_size=(3, 3),\n",
        "             kernel_initializer=\"he_normal\",\n",
        "             padding=\"same\")(x)\n",
        "  x = ELU()(x)\n",
        "  x = BatchNormalization(axis=-1)(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = Dropout(rate=0.25)(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "\n",
        "  # Next, we have two dense, ELU activated, batch-normalized layers, also followed by a dropout\n",
        "  x = Dense(units=64, kernel_initializer=\"he_normal\")(x)\n",
        "  x = ELU()(x)\n",
        "  x = BatchNormalization(axis=-1)(x)\n",
        "  x = Dropout(rate=0.5)(x)\n",
        "\n",
        "  x = Dense(units=64, kernel_initializer=\"he_normal\")(x)\n",
        "  x = ELU()(x)\n",
        "  x = BatchNormalization(axis=-1)(x)\n",
        "  x = Dropout(rate=0.5)(x)\n",
        "\n",
        "  # Finally, we encounter the output layer, with as many neurons as classes in the dataset. \n",
        "  # Of course, it's softmax-activated:\n",
        "  x = Dense(units=classes, kernel_initializer=\"he_normal\")(x)\n",
        "  output = Softmax()(x)\n",
        "\n",
        "  return Model(input, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPE1kpj1S0B-"
      },
      "source": [
        "The `load_dataset()` loads both the images and labels for the training, validation, and test datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0ma3BXbSMjF"
      },
      "source": [
        "def load_dataset(dataset_path, classes):\n",
        "  train_images = []\n",
        "  train_labels = []\n",
        "  val_images = []\n",
        "  val_labels = []\n",
        "  test_images = []\n",
        "  test_labels = []\n",
        "\n",
        "  \"\"\"\n",
        "  Let's parse the emotion column first. Although the dataset\n",
        "  contains faces for seven classes, we'll combine disgust and angry (encoded as 0 and 1, respectively) \n",
        "  because both share most of the facial features, and merging them leads to better results\n",
        "  \"\"\"\n",
        "  with open(dataset_path, \"r\") as f:\n",
        "    reader = csv.DictReader(f)\n",
        "\n",
        "    for line in reader:\n",
        "      label = int(line[\"emotion\"])\n",
        "      if label <= 1:\n",
        "        label = 0   # This merges classes 1 and 0\n",
        "      if label > 0:\n",
        "        label -= 1  # All classes start from 0\n",
        "\n",
        "      \"\"\"\n",
        "      Next, we parse the pixels column, which is 2,034 whitespace-separated integers,\n",
        "      corresponding to the grayscale pixels for the image (48x48=2034)\n",
        "      \"\"\"\n",
        "      image = np.array(line[\"pixels\"].split(\" \"), dtype=\"uint8\")\n",
        "      image = image.reshape((48, 48))\n",
        "      image = img_to_array(image)\n",
        "\n",
        "      \"\"\"\n",
        "      Now, to figure out to which subset this image and label belong, we must look at the Usage column.\n",
        "      \"\"\"\n",
        "      if line[\"Usage\"] == \"Training\":\n",
        "        train_images.append(image)\n",
        "        train_labels.append(label)\n",
        "      elif line[\"Usage\"] == \"PrivateTest\":\n",
        "        val_images.append(image)\n",
        "        val_labels.append(label)\n",
        "      else:\n",
        "        test_images.append(image)\n",
        "        test_labels.append(label)\n",
        "\n",
        "  # Convert all the images to NumPy arrays\n",
        "  train_images = np.array(train_images)\n",
        "  val_images = np.array(val_images)\n",
        "  test_images = np.array(test_images)\n",
        "\n",
        "  # Then, one-hot encode all the labels\n",
        "  train_labels = to_categorical(np.array(train_labels), classes)\n",
        "  val_labels = to_categorical(np.array(val_labels), classes)\n",
        "  test_labels = to_categorical(np.array(test_labels), classes)\n",
        "  \n",
        "  # Return all the images and labels\n",
        "  return (train_images, train_labels), (val_images, val_labels), (test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV1YyZp3TitU"
      },
      "source": [
        "Now, let's define a function to compute the area of a rectangle. We'll use this later to get the largest face detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbudgriITTnu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "7559b74d-f275-4164-ebc9-0a4dc48728f7"
      },
      "source": [
        "base = \"fruits\"\n",
        "for subset in [\"test\", \"train\"]:\n",
        "  folder = os.path.sep.join([f\"{subset}_zip\", subset])\n",
        "  labels_path = os.path.sep.join([f\"{subset}_labels.csv\"])\n",
        "\n",
        "  bboxes_df = bboxes_to_csv(folder)\n",
        "  bboxes_df.to_csv(labels_path, index=None)\n",
        "\n",
        "  # Then, use the same labels to produce the tf.train.Examples corresponding to the current subset of data being processed:\n",
        "  writer = (tf.python_io.TFRecordWriter(f\"resources/{subset}.record\"))\n",
        "  examples = pd.read_csv(f\"fruits/{subset}_labels.csv\")\n",
        "  grouped = split(examples, \"filename\")\n",
        "\n",
        "  path = os.path.join(f\"fruits/{subset}_zip/{subset}\")\n",
        "  for group in grouped:\n",
        "    tf_example = create_tf_example(group, path)\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "  writer.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-278a43a58fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Then, use the same labels to produce the tf.train.Examples corresponding to the current subset of data being processed:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"resources/{subset}.record\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"fruits/{subset}_labels.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mgrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"filename\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/tf_record.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, options)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     super(TFRecordWriter, self).__init__(\n\u001b[0;32m--> 299\u001b[0;31m         compat.as_bytes(path), options._as_record_writer_options())\n\u001b[0m\u001b[1;32m    300\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: resources/test.record; No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x8sbSJJVNIP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}