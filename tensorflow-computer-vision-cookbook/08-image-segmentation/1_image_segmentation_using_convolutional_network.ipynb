{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-image-segmentation-using-convolutional-network.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNLZlGBUUTCKSM+ZVpL4gQJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/tensorflow-computer-vision-cookbook/blob/main/8-image-segmentation/1_image_segmentation_using_convolutional_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrWxON_JOLLN"
      },
      "source": [
        "## Image segmentation using convolutional network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-28NuGNePDbk"
      },
      "source": [
        "Image segmentation is one of the biggest areas of study in computer vision. It consists of simplifying the visual contents of an image by grouping together pixels that share one or more defining characteristics, such as location, color, or texture. \n",
        "\n",
        "As is the case with many other subareas of computer vision, image segmentation has been greatly boosted by deep neural networks, mainly in industries such as medicine and autonomous driving.\n",
        "\n",
        "While it's great to classify the contents of an image, more often than not, it's not enough.\n",
        "\n",
        "What if we want to know exactly where an object is? \n",
        "\n",
        "What if we're interested in its shape? \n",
        "\n",
        "What if we need its contour? \n",
        "\n",
        "These fine-grained needs cannot be met with traditional classification techniques.\n",
        "\n",
        "We can frame an image segmentation problem in a very similar way to a regular classification project. \n",
        "\n",
        "How?\n",
        "\n",
        "Instead of labeling the image as a whole, we'll label each pixel! This is known as image segmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQR_tTAVQdDJ"
      },
      "source": [
        "## Image segmentation using fully convolutional network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NgmiXpwQ0s2"
      },
      "source": [
        "If you were to create your first network for image segmentation while knowing that, at its core, segmenting is just pixel-wise classification, what would you do? \n",
        "\n",
        "You would probably take a battle-tested architecture and swap the final layers (usually fully connected ones) with convolutions in order to produce an output volume, instead of an output vector.\n",
        "\n",
        "Well, that's exactly what we'll do to build a **Fully Convolutional Network (FCN)** for image segmentation based on the famous **VGG16** network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RECsDEsGRJAt"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utnpLJaDRqCl"
      },
      "source": [
        "!pip install git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsJNm7wpRKQg"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.plots\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "# Define an alias for tf.data.experimental.AUTOTUNE:\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBk3C7gH8zcn"
      },
      "source": [
        "# download VGG model weights\n",
        "!wget https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov-aL5wNSLO1"
      },
      "source": [
        "## Loading and preparing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iaye93o1SQki"
      },
      "source": [
        "Regarding the data, we will segment images from the Oxford-IIIT Pet dataset. The\n",
        "good news is that we'll access it using tensorflow-datasets, so we don't really need to do anything in that respect here. \n",
        "\n",
        "Each pixel in this dataset is classified as follows:\n",
        "- 1: The pixel belongs to a pet (cat or dog).\n",
        "- 2: The pixel belongs to the contour of a pet.\n",
        "- 3: The pixel belongs to the surroundings.\n",
        "\n",
        "We will normalize the images in the dataset to the range [0, 1].\n",
        "Just for consistency's sake, we'll subtract one from each pixel in the mask so that they go from 0 all the way to 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG4qPD7mRePj"
      },
      "source": [
        "def normalize(input_image, input_mask):\n",
        "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "  input_mask -= 1\n",
        "\n",
        "  return input_image, input_mask"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JusWMk0ITL29"
      },
      "source": [
        "Let's loads both the image and its mask, given a TensorFlow dataset element. We will seize the opportunity to resize the images to `256x256` here. Also, if the train flag is set to True, we can perform a bit of augmentation by randomly mirroring the image and its mask. Lastly, we must normalize the inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idU2FIOKTDSX"
      },
      "source": [
        "@tf.function\n",
        "def load_image(dataset, train=True):\n",
        "  input_image = tf.image.resize(dataset[\"image\"], (256, 256))\n",
        "  input_mask = tf.image.resize(dataset[\"segmentation_mask\"], (256, 256))\n",
        "\n",
        "  if train and np.random.uniform() > 0.5:\n",
        "    input_image = tf.image.flip_left_right(input_image)\n",
        "    input_mask = tf.image.flip_left_right(input_mask)\n",
        "\n",
        "  input_image, input_mask = normalize(input_image, input_mask)\n",
        "\n",
        "  return input_image, input_mask"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqq2DShA8QQx"
      },
      "source": [
        "## Create Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT6F62VsU3O0"
      },
      "source": [
        "Let's implement a class, which encapsulates all the logic required to build, train, and evaluate our FCN image segmentation model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzux2JU_UjUw"
      },
      "source": [
        "class FCN(object):\n",
        "  \n",
        "  def __init__(self, input_shape=(256, 256, 3), output_channels=3):\n",
        "    self.input_shape = input_shape \n",
        "    self.output_channels = output_channels \n",
        "\n",
        "    # defining the path to the weights of the VGG16 model\n",
        "    # self.vgg_weights_path = str(pathlib.Path.home() / \"vgg16_weights_tf_dim_\" \"ordering_tf_kernels.h5\")\n",
        "    self.vgg_weights_path = \"vgg16_weights_tf_dim_\" \"ordering_tf_kernels.h5\"\n",
        "\n",
        "    self.model = self.create_model()\n",
        "\n",
        "    loss = SparseCategoricalCrossentropy(from_logits=True)\n",
        "    self.model.compile(optimizer=RMSprop(), loss=loss, metrics=[\"accuracy\"])\n",
        "\n",
        "  def create_model(self):\n",
        "    input = Input(shape=self.input_shape)\n",
        "\n",
        "    # first block of convolutions and max pooling layers\n",
        "    x = Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", name=\"block1_conv1\")(input)\n",
        "    x = Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", name=\"block1_conv2\")(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=2, name=\"block1_pool\")(x)\n",
        "\n",
        "    # second block of convolutions and max pooling layers\n",
        "    x = Conv2D(filters=128, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", name=\"block2_conv1\")(x)\n",
        "    x = Conv2D(filters=128, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", name=\"block2_conv2\")(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=2, name=\"block2_pool\")(x)\n",
        "\n",
        "    # third block of convolutions and max pooling layers\n",
        "    x = Conv2D(filters=256, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv1\")(x)\n",
        "    x = Conv2D(filters=256, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv2\")(x)\n",
        "    x = Conv2D(filters=256, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv3\")(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=2, name=\"block3_pool\")(x)\n",
        "\n",
        "    block3_pool = x\n",
        "\n",
        "    # fourth block of convolutions and max pooling layers\n",
        "    x = Conv2D(filters=512, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", name=\"block4_conv1\")(x)\n",
        "    x = Conv2D(filters=512, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", name=\"block4_conv2\")(x)\n",
        "    x = Conv2D(filters=512, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", name=\"block4_conv3\")(x)\n",
        "    block4_pool = MaxPooling2D(pool_size=(2, 2), strides=2, name=\"block4_pool\")(x)\n",
        "\n",
        "    # The fifth block is a repetition of block four, again with 512 filter-deep convolutions\n",
        "    x = Conv2D(filters=512, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", name=\"block5_conv1\")(block4_pool)\n",
        "    x = Conv2D(filters=512, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", name=\"block5_conv2\")(x)\n",
        "    x = Conv2D(filters=512, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", name=\"block5_conv3\")(x)\n",
        "    block5_pool = MaxPooling2D(pool_size=(2, 2), strides=2, name=\"block5_pool\")(x)\n",
        "\n",
        "    model = Model(input, block5_pool)\n",
        "    model.load_weights(self.vgg_weights_path, by_name=True)\n",
        "\n",
        "    output = Conv2D(filters=self.output_channels, kernel_size=(7, 7), activation=\"relu\", padding=\"same\", name=\"conv6\")(block5_pool)\n",
        "\n",
        "    \"\"\"\n",
        "    output, in a traditional VGG16 architecture, is comprised of fully connected layers. However, we'll be replacing them \n",
        "    with transposed convolutions. Notice we are connecting these layers to the output of the fifth block\n",
        "    \"\"\"\n",
        "    conv6_4 = Conv2DTranspose(filters=self.output_channels, kernel_size=(4, 4), strides=4, use_bias=False)(output)\n",
        "    # Create a 1x1 convolution and connect it to the output of the fourth block(this is, indeed, a skip connection)\n",
        "    pool4_n = Conv2D(filters=self.output_channels, kernel_size=(1, 1), activation=\"relu\", padding=\"same\", name=\"pool4_n\")(block4_pool)\n",
        "    pool4_n_2 = Conv2DTranspose(filters=self.output_channels, kernel_size=(2, 2), strides=2, use_bias=False)(pool4_n)\n",
        "    # Pass the output of the third block through a 1x1 convolution\n",
        "    pool3_n = Conv2D(filters=self.output_channels, kernel_size=(1, 1), activation=\"relu\", padding=\"same\", name=\"pool3_n\")(block3_pool)\n",
        "\n",
        "    # Then, merge these three paths into one and pass them through a final transposed convolution.\n",
        "    output = Add(name=\"add\")([pool4_n_2, pool3_n, conv6_4])\n",
        "    output = Conv2DTranspose(filters=self.output_channels, kernel_size=(8, 8), strides=8, use_bias=False)(output)\n",
        "\n",
        "    output = Softmax()(output)\n",
        "\n",
        "    return Model(input, output)\n",
        "\n",
        "  @staticmethod\n",
        "  def plot_model_history(model_history, metric, ylim=True):\n",
        "    plt.style.use('seaborn-darkgrid')\n",
        "    plotter = tfdocs.plots.HistoryPlotter()\n",
        "    plotter.plot({'Model': model_history}, metric=metric)\n",
        "\n",
        "    plt.title(f'{metric.upper()}')\n",
        "    if ylim is None:\n",
        "        plt.ylim([0, 1])\n",
        "    else:\n",
        "        plt.ylim(ylim)\n",
        "\n",
        "    plt.savefig(f'{metric}.png')\n",
        "    plt.close()\n",
        "\n",
        "  def train(self, train_dataset, epochs, steps_per_epoch, validation_dataset, validation_steps):\n",
        "    hist = self.model.fit(train_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, validation_data=validation_dataset)\n",
        "\n",
        "    self.plot_model_history(hist, 'loss', [0., 2.0])\n",
        "    self.plot_model_history(hist, 'accuracy')\n",
        "\n",
        "  @staticmethod\n",
        "  def process_mask(mask):\n",
        "    \"\"\"\n",
        "    It is used to make the segmentation masks compatible with OpenCV. What this function does is create a three-channeled\n",
        "    version of a grayscale mask and upscale the class values to the [0, 255] range\n",
        "    \"\"\"\n",
        "    mask = (mask.numpy() * 127.5).astype(\"uint8\")\n",
        "    mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    return mask\n",
        "\n",
        "  def save_image_and_masks(self, image, ground_truth_mask, prediction_mask, image_id):\n",
        "    \"\"\"\n",
        "    This method creates a mosaic of the original image, the ground truth mask, and the predicted segmentation mask, \n",
        "    and then saves it to disk for later revision\n",
        "    \"\"\"\n",
        "    image = (image.numpy() * 255.0).astype('uint8')\n",
        "    gt_mask = self.process_mask(ground_truth_mask)\n",
        "    pred_mask = self.process_mask(prediction_mask)\n",
        "\n",
        "    mosaic = np.hstack([image, gt_mask, pred_mask])\n",
        "    mosaic = cv2.cvtColor(mosaic, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    cv2.imwrite(f'mosaic_{image_id}.jpg', mosaic)\n",
        "\n",
        "  @staticmethod\n",
        "  def create_mask(prediction_mask):\n",
        "    \"\"\"\n",
        "    In order to pass the output volume produced by the network to a valid segmentation mask, we must take the index\n",
        "    with the highest value at each pixel location. This corresponds to the most likely category for that pixel.\n",
        "    \"\"\"\n",
        "    prediction_mask = tf.argmax(prediction_mask, axis=-1)\n",
        "    prediction_mask = prediction_mask[..., tf.newaxis]\n",
        "\n",
        "    return prediction_mask[0]\n",
        "\n",
        "  def save_predictions(self, dataset, sample_size=1):\n",
        "    \"\"\"\n",
        "    This method uses the FCN to predict the mask of a sample of images in the input dataset. It then saves the result to disk.\n",
        "    \"\"\"\n",
        "    for id, (image, mask) in enumerate(dataset.take(sample_size), start=1):\n",
        "      pred_mask = self.model.predict(image)\n",
        "      pred_mask = self.create_mask(pred_mask)\n",
        "\n",
        "      image = image[0]\n",
        "      ground_truth_mask = mask[0]\n",
        "\n",
        "      self.save_image_and_masks(image, ground_truth_mask, pred_mask, image_id=id)\n",
        "\n",
        "  def evaluate(self, test_dataset, sample_size=5):\n",
        "    result = self.model.evaluate(test_dataset)\n",
        "    print(f\"Accuracy: {result[1] * 100:.2f}%\")\n",
        "\n",
        "    self.save_predictions(test_dataset, sample_size)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmoG7SpD8ZeO"
      },
      "source": [
        "## Putting all stuffs together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-WJdOKL5KSi"
      },
      "source": [
        "Now let's download (or load, if cached) Oxford IIIT Pet Dataset, along with its\n",
        "metadata, using TensorFlow Datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3m0Rci84zlj"
      },
      "source": [
        "dataset, info = tfds.load(\"oxford_iiit_pet\", with_info=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6mpmoOr5b3p"
      },
      "source": [
        "Now we use the metadata to define the corresponding number of steps the network will take over the training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFdTvgBL5VaB"
      },
      "source": [
        "TRAIN_SIZE = info.splits['train'].num_examples\n",
        "VALIDATION_SIZE = info.splits['test'].num_examples\n",
        "BATCH_SIZE = 32\n",
        "STEPS_PER_EPOCH = TRAIN_SIZE // BATCH_SIZE\n",
        "\n",
        "VALIDATION_SUBSPLITS = 5\n",
        "VALIDATION_STEPS = VALIDATION_SIZE // BATCH_SIZE\n",
        "VALIDATION_STEPS //= VALIDATION_SUBSPLITS\n",
        "\n",
        "BUFFER_SIZE = 1000"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_hfZBo95oSO"
      },
      "source": [
        "Let's define the training and testing datasets' pipelines:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlH2BmF85k8b"
      },
      "source": [
        "train_dataset = (dataset[\"train\"].map(load_image, num_parallel_calls=AUTOTUNE)\n",
        "                                 .cache()\n",
        "                                 .shuffle(BUFFER_SIZE)\n",
        "                                 .batch(BATCH_SIZE)\n",
        "                                 .repeat()\n",
        "                                 .prefetch(buffer_size=AUTOTUNE))\n",
        "test_dataset = (dataset[\"test\"].map(lambda x: load_image(x, train=False), num_parallel_calls=AUTOTUNE)\n",
        "                               .batch(BATCH_SIZE))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqh9u8mj6n1w"
      },
      "source": [
        "Now we will instantiate the FCN and train it for 120 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trr6YMZ46fxU",
        "outputId": "c0c60887-95d5-47fd-d938-8d2534abca20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fcn = FCN(output_channels=3)\n",
        "fcn.train(train_dataset, epochs=120, steps_per_epoch=STEPS_PER_EPOCH, validation_steps=VALIDATION_STEPS, validation_dataset=test_dataset)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "115/115 [==============================] - 172s 1s/step - loss: 1.2119 - accuracy: 0.3324 - val_loss: 1.2131 - val_accuracy: 0.3326\n",
            "Epoch 2/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 3/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 4/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 5/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 6/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 7/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 8/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 9/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 10/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 11/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 12/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 13/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 14/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 15/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 16/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 17/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 18/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 19/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 20/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 21/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 22/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 23/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 24/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 25/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 26/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 27/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 28/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 29/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 30/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 31/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 32/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 33/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 34/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 35/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 36/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 37/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 38/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 39/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 40/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 41/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 42/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 43/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 44/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 45/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 46/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 47/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 48/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 49/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 50/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 51/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 52/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 53/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 54/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 55/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 56/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 57/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 58/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 59/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 60/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 61/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 62/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 63/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 64/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 65/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 66/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 67/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 68/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 69/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 70/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 71/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 72/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 73/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 74/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 75/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 76/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 77/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 78/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 79/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 80/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 81/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 82/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 83/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 84/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 85/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 86/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 87/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 88/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 89/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 90/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 91/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 92/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 93/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 94/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 95/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 96/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 97/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 98/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 99/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 100/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 101/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 102/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 103/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 104/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 105/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 106/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 107/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 108/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 109/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 110/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 111/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 112/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 113/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 114/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 115/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 116/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 117/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 118/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 119/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n",
            "Epoch 120/120\n",
            "115/115 [==============================] - 122s 1s/step - loss: 1.2126 - accuracy: 0.3324 - val_loss: 1.2129 - val_accuracy: 0.3328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_n1Ht-M7FRU"
      },
      "source": [
        "Lastly, evaluate the network on the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7VMVAqb6_T3",
        "outputId": "c5a01afa-d51b-4492-c209-3bec41d63fb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fcn.evaluate(test_dataset)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "115/115 [==============================] - 37s 317ms/step - loss: 1.2129 - accuracy: 0.3329\n",
            "Accuracy: 33.29%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXFtI1Ds7Yzj"
      },
      "source": [
        "The training curves display a healthy behavior, meaning that the network did,\n",
        "indeed, learn. \n",
        "\n",
        "However, the true test is to visually assess the results:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MF6I3BD74Yy"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxZYraCp765P"
      },
      "source": [
        "In this recipe, we implemented an FCN for image segmentation. Even though we adapted a well-known architecture, VGG16, to our purposes, in reality, there are many different adaptations of FCNs that extend or modify other seminal architectures, such as ResNet50, DenseNet, and other variants of VGG.\n",
        "\n",
        "What we need to remember is that FCN is more of a template than a concrete\n",
        "implementation. Such a template consists of swapping the fully connected layers at the end of these networks, which are often used for traditional image classification, with 1x1 convolutions and upsampling layers (either UpSampling2D() with bilinear interpolation or ConvTranspose2D()). The achieved result is that instead of classifying the whole image with an output vector of probabilities, we produce an output volume that has the same dimensions as the input image, where each pixel contains a probability distribution of the classes it can belong to. Such an output volume of pixel-wise likelihood\n",
        "is known as a predicted segmentation mask."
      ]
    }
  ]
}