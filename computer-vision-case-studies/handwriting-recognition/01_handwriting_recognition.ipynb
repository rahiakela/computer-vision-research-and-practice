{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-handwriting-recognition.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6N3Q2nM9lrnCWlxgFZn3N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/computer-vision-research-and-practice/blob/main/computer-vision-case-studies/handwriting-recognition/01_handwriting_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-4qE3wQE0CC"
      },
      "source": [
        "##Handwriting recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHbf_-2zE0oY"
      },
      "source": [
        "**Authors:** [A_K_Nain](https://twitter.com/A_K_Nain), [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
        "**Date created:** 2021/08/16<br>\n",
        "**Last modified:** 2021/08/16<br>\n",
        "**Description:** Training a handwriting recognition model with variable-length sequences.\n",
        "\n",
        "**Blog reference:** https://keras.io/examples/vision/handwriting_recognition/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIeQQ9j8E9Nn"
      },
      "source": [
        "##Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kruITvAnE_7E"
      },
      "source": [
        "This example shows how the [Captcha OCR](https://keras.io/examples/vision/captcha_ocr/)\n",
        "example can be extended to the\n",
        "[IAM Dataset](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database),\n",
        "which has variable length ground-truth targets. Each sample in the dataset is an image of some\n",
        "handwritten text, and its corresponding target is the string present in the image.\n",
        "The IAM Dataset is widely used across many OCR benchmarks, so we hope this example can serve as a\n",
        "good starting point for building OCR systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NuLsZ_zF2YB"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfV110BTF1nb"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jdn9sbPFGOo"
      },
      "source": [
        "##Data collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWdamjktFG4i",
        "outputId": "b47f2c17-43fc-4de1-d6b7-7b1f3ab7566b"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget -q https://git.io/J0fjL -O IAM_Words.zip\n",
        "unzip -qq IAM_Words.zip\n",
        "\n",
        "mkdir data\n",
        "mkdir data/words\n",
        "tar -xf IAM_Words/words.tgz -C data/words\n",
        "mv IAM_Words/words.txt data\n",
        "\n",
        "rm -rf IAM_Words.zip\n",
        "rm -rf IAM_Words"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0LNHuVWFqvT"
      },
      "source": [
        "Preview how the dataset is organized. Lines prepended by \"#\" are just metadata information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5rBgwcoFrdd",
        "outputId": "85dd15a0-fb74-4a81-b744-325e7b9beac2"
      },
      "source": [
        "!head -20 data/words.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#--- words.txt ---------------------------------------------------------------#\n",
            "#\n",
            "# iam database word information\n",
            "#\n",
            "# format: a01-000u-00-00 ok 154 1 408 768 27 51 AT A\n",
            "#\n",
            "#     a01-000u-00-00  -> word id for line 00 in form a01-000u\n",
            "#     ok              -> result of word segmentation\n",
            "#                            ok: word was correctly\n",
            "#                            er: segmentation of word can be bad\n",
            "#\n",
            "#     154             -> graylevel to binarize the line containing this word\n",
            "#     1               -> number of components for this word\n",
            "#     408 768 27 51   -> bounding box around this word in x,y,w,h format\n",
            "#     AT              -> the grammatical tag for this word, see the\n",
            "#                        file tagset.txt for an explanation\n",
            "#     A               -> the transcription for this word\n",
            "#\n",
            "a01-000u-00-00 ok 154 408 768 27 51 AT A\n",
            "a01-000u-00-01 ok 154 507 766 213 48 NN MOVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcsO4QfQFwJ1"
      },
      "source": [
        "##Dataset splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSfdrxEDH_Ld",
        "outputId": "fb3cfdbe-dd3d-4eb1-bf68-f32f3177e6a9"
      },
      "source": [
        "base_path = \"data\"\n",
        "words_list = []\n",
        "\n",
        "words = open(f\"{base_path}/words.txt\", \"r\").readlines()\n",
        "for line in words:\n",
        "  if line[0] == \"#\":\n",
        "    continue\n",
        "  # We don't need to deal with errored entries\n",
        "  if line.split(\" \")[1] != \"err\":\n",
        "    words_list.append(line)\n",
        "  \n",
        "print(len(words_list))\n",
        "np.random.shuffle(words_list)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTRvHaoKJL4p"
      },
      "source": [
        "We will split the dataset into three subsets with a 90:5:5 ratio (train:validation:test)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkS-3RZEJND7",
        "outputId": "09a1abb5-a892-479a-e4f0-77372e2d08ad"
      },
      "source": [
        "split_index = int(0.9 * len(words_list))\n",
        "train_samples = words_list[:split_index]\n",
        "test_samples = words_list[split_index:]\n",
        "\n",
        "val_split_index = int(0.5 * len(test_samples))\n",
        "validation_samples = test_samples[:val_split_index]\n",
        "test_samples = test_samples[val_split_index:]\n",
        "\n",
        "assert len(words_list) == len(train_samples) + len(validation_samples) + len(test_samples)\n",
        "\n",
        "print(f\"Total training samples: {len(train_samples)}\")\n",
        "print(f\"Total validation samples: {len(validation_samples)}\")\n",
        "print(f\"Total test samples: {len(test_samples)}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training samples: 86810\n",
            "Total validation samples: 4823\n",
            "Total test samples: 4823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples[:10]"
      ],
      "metadata": {
        "id": "mF4ICtb7OisX",
        "outputId": "47ee89a5-1f9e-4e53-db3f-4a9de50186ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['r06-076-07-06 ok 177 1807 2010 76 53 CC or\\n',\n",
              " 'n01-004-01-01 ok 180 614 906 246 69 JJ unable\\n',\n",
              " 'g06-011f-00-03 ok 203 778 721 46 70 INO of\\n',\n",
              " 'f04-011-07-01 ok 145 504 1976 118 78 BEDZ was\\n',\n",
              " 'e04-103-01-01 ok 174 471 916 205 123 VB plank\\n',\n",
              " 'g06-047g-04-05 ok 182 924 1430 193 67 NP Europe\\n',\n",
              " 'm06-056-04-11 ok 158 2061 1537 11 21 , ,\\n',\n",
              " 'j06-026-03-04 ok 185 1593 1416 341 129 NN sunlight\\n',\n",
              " 'm06-019-01-12 ok 189 1837 949 142 50 CD three\\n',\n",
              " 'a04-043-02-05 ok 186 1906 1113 59 68 INO of\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK7bL7RONZR3"
      },
      "source": [
        "##Data input pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5u25EEzNZ3d"
      },
      "source": [
        "We start building our data input pipeline by first preparing the image paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVYBxBsqNcBR"
      },
      "source": [
        "base_image_path = os.path.join(base_path, \"words\")\n",
        "\n",
        "def get_image_paths_and_labels(samples):\n",
        "  paths = []\n",
        "  corrected_samples = []\n",
        "\n",
        "  for (i, file_line) in enumerate(samples):\n",
        "    line_split = file_line.strip()\n",
        "    line_split = line_split.split(\" \")\n",
        "    \n",
        "    # Each line split will have this format for the corresponding image:\n",
        "    # part1/part1-part2/part1-part2-part3.png\n",
        "    image_name = line_split[0]\n",
        "    partI = image_name.split(\"-\")[0]\n",
        "    partII = image_name.split(\"-\")[1]\n",
        "    img_path = os.path.join(base_image_path, partI, partI +\"-\"+ partII, image_name + \".png\")\n",
        "\n",
        "    if os.path.getsize(img_path):\n",
        "      paths.append(img_path)\n",
        "      corrected_samples.append(file_line.split(\"\\n\")[0])\n",
        "\n",
        "  return paths, corrected_samples"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_paths, train_labels = get_image_paths_and_labels(train_samples)\n",
        "validation_img_paths, validation_labels = get_image_paths_and_labels(validation_samples)\n",
        "test_img_paths, test_labels = get_image_paths_and_labels(test_samples)"
      ],
      "metadata": {
        "id": "11cnEW_gLk5d"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we prepare the ground-truth labels."
      ],
      "metadata": {
        "id": "OplongROO4mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find maximum length and the size of the vocabulary in the training data.\n",
        "train_labels_cleaned = []\n",
        "characters = set()\n",
        "max_len = 0\n",
        "\n",
        "for label in train_labels:\n",
        "  label = label.split(\" \")[-1].strip()\n",
        "  for char in label:\n",
        "    characters.add(char)\n",
        "  \n",
        "  max_len = max(max_len, len(label))\n",
        "  train_labels_cleaned.append(label)\n",
        "\n",
        "print(\"Maximum length: \", max_len)\n",
        "print(\"Vocab size: \", len(characters))\n",
        "\n",
        "# Check some label samples\n",
        "train_labels_cleaned[:10]"
      ],
      "metadata": {
        "id": "w8UsU1cMOwWN",
        "outputId": "8f60291b-245a-4c1b-9c3e-d9e935f289d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length:  21\n",
            "Vocab size:  78\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['or',\n",
              " 'unable',\n",
              " 'of',\n",
              " 'was',\n",
              " 'plank',\n",
              " 'Europe',\n",
              " ',',\n",
              " 'sunlight',\n",
              " 'three',\n",
              " 'of']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we clean the validation and the test labels as well."
      ],
      "metadata": {
        "id": "Zw8syS6wRQFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_labels(labels):\n",
        "  cleaned_labels = []\n",
        "  for label in labels:\n",
        "    label = label.split(\" \")[-1].strip()\n",
        "    cleaned_labels.append(label)\n",
        "  return cleaned_labels"
      ],
      "metadata": {
        "id": "gyO197e7RQgo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_labels_cleaned = clean_labels(validation_labels)\n",
        "test_labels_cleaned = clean_labels(test_labels)"
      ],
      "metadata": {
        "id": "2pZ8B7JeRic7"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building the character vocabulary"
      ],
      "metadata": {
        "id": "la172YMJR7ER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-2YhN-5PR7_C"
      }
    }
  ]
}