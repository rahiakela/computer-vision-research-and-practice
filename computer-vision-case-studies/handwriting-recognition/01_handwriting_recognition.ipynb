{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-handwriting-recognition.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNcQBb3dZwL/0XZsJShpcb6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/computer-vision-research-and-practice/blob/main/computer-vision-case-studies/handwriting-recognition/01_handwriting_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-4qE3wQE0CC"
      },
      "source": [
        "##Handwriting recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHbf_-2zE0oY"
      },
      "source": [
        "**Authors:** [A_K_Nain](https://twitter.com/A_K_Nain), [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
        "**Date created:** 2021/08/16<br>\n",
        "**Last modified:** 2021/08/16<br>\n",
        "**Description:** Training a handwriting recognition model with variable-length sequences.\n",
        "\n",
        "**Blog reference:** https://keras.io/examples/vision/handwriting_recognition/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIeQQ9j8E9Nn"
      },
      "source": [
        "##Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kruITvAnE_7E"
      },
      "source": [
        "This example shows how the [Captcha OCR](https://keras.io/examples/vision/captcha_ocr/)\n",
        "example can be extended to the\n",
        "[IAM Dataset](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database),\n",
        "which has variable length ground-truth targets. Each sample in the dataset is an image of some\n",
        "handwritten text, and its corresponding target is the string present in the image.\n",
        "The IAM Dataset is widely used across many OCR benchmarks, so we hope this example can serve as a\n",
        "good starting point for building OCR systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NuLsZ_zF2YB"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfV110BTF1nb"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jdn9sbPFGOo"
      },
      "source": [
        "##Data collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWdamjktFG4i",
        "outputId": "2c1491cf-f7af-42dd-883c-c37ae5e627a0"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget -q https://git.io/J0fjL -O IAM_Words.zip\n",
        "unzip -qq IAM_Words.zip\n",
        "\n",
        "mkdir data\n",
        "mkdir data/words\n",
        "tar -xf IAM_Words/words.tgz -C data/words\n",
        "mv IAM_Words/words.txt data\n",
        "\n",
        "rm -rf IAM_Words.zip\n",
        "rm -rf IAM_Words"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0LNHuVWFqvT"
      },
      "source": [
        "Preview how the dataset is organized. Lines prepended by \"#\" are just metadata information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5rBgwcoFrdd",
        "outputId": "0f342f46-39f0-4841-d301-c9886c152474"
      },
      "source": [
        "!head -20 data/words.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#--- words.txt ---------------------------------------------------------------#\n",
            "#\n",
            "# iam database word information\n",
            "#\n",
            "# format: a01-000u-00-00 ok 154 1 408 768 27 51 AT A\n",
            "#\n",
            "#     a01-000u-00-00  -> word id for line 00 in form a01-000u\n",
            "#     ok              -> result of word segmentation\n",
            "#                            ok: word was correctly\n",
            "#                            er: segmentation of word can be bad\n",
            "#\n",
            "#     154             -> graylevel to binarize the line containing this word\n",
            "#     1               -> number of components for this word\n",
            "#     408 768 27 51   -> bounding box around this word in x,y,w,h format\n",
            "#     AT              -> the grammatical tag for this word, see the\n",
            "#                        file tagset.txt for an explanation\n",
            "#     A               -> the transcription for this word\n",
            "#\n",
            "a01-000u-00-00 ok 154 408 768 27 51 AT A\n",
            "a01-000u-00-01 ok 154 507 766 213 48 NN MOVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcsO4QfQFwJ1"
      },
      "source": [
        "##Dataset splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSfdrxEDH_Ld",
        "outputId": "82fdf972-9526-4f11-d146-fab855fb7f78"
      },
      "source": [
        "base_path = \"data\"\n",
        "words_list = []\n",
        "\n",
        "words = open(f\"{base_path}/words.txt\", \"r\").readlines()\n",
        "for line in words:\n",
        "  if line[0] == \"#\":\n",
        "    continue\n",
        "  # We don't need to deal with errored entries\n",
        "  if line.split(\" \")[1] != \"err\":\n",
        "    words_list.append(list)\n",
        "  \n",
        "print(len(words_list))\n",
        "np.random.shuffle(words_list)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTRvHaoKJL4p"
      },
      "source": [
        "We will split the dataset into three subsets with a 90:5:5 ratio (train:validation:test)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkS-3RZEJND7",
        "outputId": "cd85679d-c474-45c0-b7fd-8b5068e755aa"
      },
      "source": [
        "split_index = int(0.9 * len(words_list))\n",
        "train_samples = words_list[:split_index]\n",
        "test_samples = words_list[split_index:]\n",
        "\n",
        "val_split_index = int(0.5 * len(test_samples))\n",
        "validation_samples = test_samples[:val_split_index]\n",
        "test_samples = test_samples[val_split_index:]\n",
        "\n",
        "assert len(words_list) == len(train_samples) + len(validation_samples) + len(test_samples)\n",
        "\n",
        "print(f\"Total training samples: {len(train_samples)}\")\n",
        "print(f\"Total validation samples: {len(validation_samples)}\")\n",
        "print(f\"Total test samples: {len(test_samples)}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training samples: 86810\n",
            "Total validation samples: 4823\n",
            "Total test samples: 4823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK7bL7RONZR3"
      },
      "source": [
        "##Data input pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5u25EEzNZ3d"
      },
      "source": [
        "We start building our data input pipeline by first preparing the image paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVYBxBsqNcBR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}