{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "implementing-YOLOV1-from-scratch-using-Keras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPQElWX1YXVi+bBc2zY9r1C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/computer-vision-research-and-practice/blob/main/computer-vision-case-studies/object-detections/yolo-implementations/yolo-v1/implementing_YOLOV1_from_scratch_using_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementing YOLOV1 from scratch using Keras"
      ],
      "metadata": {
        "id": "dSO4Z9LxSJ5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook I am going to implement YOLOV1 as described in the paper [You Only Look Once](https://arxiv.org/abs/1506.02640). The goal is to replicate the model as described in the paper and in the process, understand the nuances of using Keras on a complex problem.\n",
        "\n",
        "<img src='https://www.maskaravivek.com/post/yolov1/featured_hu2959f475cef1ef9098f72ca1a1294bd8_186245_720x0_resize_lanczos_2.png?raw=1' width='800'/>\n",
        "\n",
        "**Reference**\n",
        "\n",
        "[Implementing YOLOV1 from scratch using Keras Tensorflow 2.0](https://www.maskaravivek.com/post/yolov1/)"
      ],
      "metadata": {
        "id": "Q6OvGZNcSSLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "ZEE1vBosSZgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow import keras\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, InputLayer, Dropout, Flatten, Reshape\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "import argparse\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt    # for plotting the images\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "c8PRQdAYSaut"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would be using [VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/) dataset as its size is manageable so it would be easy to run it using Google Colab.\n",
        "\n",
        "First, I download and extract the dataset."
      ],
      "metadata": {
        "id": "2YtkhLOAWN1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
        "\n",
        "!tar xvf VOCtrainval_06-Nov-2007.tar\n",
        "!tar xvf VOCtest_06-Nov-2007.tar\n",
        "\n",
        "!rm VOCtrainval_06-Nov-2007.tar\n",
        "!rm VOCtest_06-Nov-2007.tar"
      ],
      "metadata": {
        "id": "-7YsLUXKWZ4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preprocessing"
      ],
      "metadata": {
        "id": "bGVvdeQUWgSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we process the annotations and write the labels in a text file. A text file is easier to consume as compared to XML."
      ],
      "metadata": {
        "id": "yyUDdTjeWgup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='Build Annotations.')\n",
        "parser.add_argument('dir', default='..', help='Annotations.')\n",
        "\n",
        "sets = [('2007', 'train'), ('2007', 'val'), ('2007', 'test')]\n",
        "\n",
        "classes_num = {\n",
        "    'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5,\n",
        "    'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11,\n",
        "    'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16,\n",
        "    'sofa': 17, 'train': 18, 'tvmonitor': 19\n",
        "}"
      ],
      "metadata": {
        "id": "1yG292gHWne2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_annotation(year, image_id, f):\n",
        "  in_file = os.path.join(\"VOCdevkit/VOC%s/Annotations/%s.xml\" % (year, image_id))\n",
        "  tree = ET.parse(in_file)\n",
        "  root = tree.getroot()\n",
        "\n",
        "  for obj in root.iter(\"object\"):\n",
        "    difficult = obj.find(\"difficult\").text\n",
        "    cls = obj.find(\"name\").text\n",
        "    classes = list(classes_num.keys())\n",
        "\n",
        "    if cls not in classes or int(difficult) == 1:\n",
        "      continue\n",
        "\n",
        "    cls_id = classes.index(cls)\n",
        "    xmlbox = obj.find(\"bndbox\")\n",
        "\n",
        "    b = (int(xmlbox.find(\"xmin\").text), int(xmlbox.find(\"ymin\").text), int(xmlbox.find(\"xmax\").text), int(xmlbox.find(\"ymax\").text))\n",
        "    f.write(\" \" + \",\".join([str(a) for a in b]) + \",\" + str(cls_id))"
      ],
      "metadata": {
        "id": "17kQ2ofJLclD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for year, image_set in sets:\n",
        "  print(year, image_set)\n",
        "  with open(os.path.join(\"VOCdevkit/VOC%s/ImageSets/Main/%s.txt\" % (year, image_set)), \"r\") as f:\n",
        "    image_ids = f.read().strip().split()\n",
        "  with open(os.path.join(\"VOCdevkit\", \"%s_%s.txt\" % (year, image_set)), \"w\") as f:\n",
        "    for image_id in image_ids:\n",
        "      f.write(\"%s/VOC%s/JPEGImages/%s.jpg\" % (\"VOCdevkit\", year, image_id))\n",
        "      convert_annotation(year, image_id, f)\n",
        "      f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "qjme9VkmNC-H",
        "outputId": "a09c7000-a1f2-4e33-fd9a-936ad0608b25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2007 train\n",
            "2007 val\n",
            "2007 test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I am adding a function to prepare the input and the output. The input is a `(448, 448, 3)` image and the output is a `(7, 7, 30)` tensor. The output is based on `S x S x (B * 5 + C)`.\n",
        "\n",
        "`S X S` is the number of grids, `B` is the number of bounding boxes per grid `C` is the number of predictions per grid."
      ],
      "metadata": {
        "id": "pJU0qZcbRo66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read(image_path, label):\n",
        "  image = cv.imread(image_path)\n",
        "  image = cv.cvtColor(image, cv.COLOR_BAYER_BG2RGB)\n",
        "  image_h, image_w = image.shape[0:2]\n",
        "  image = cv.resize(image, (448, 448))\n",
        "  image = image / 255.\n",
        "\n",
        "  label_matrix = np.zeros([7, 7, 30])\n",
        "  for l in label:\n",
        "    l = l.split(\",\")\n",
        "    l = np.array(l, dtype=np.int)\n",
        "\n",
        "    xmin = l[0]\n",
        "    ymin = l[1]\n",
        "    xmax = l[2]\n",
        "    ymax = l[3]\n",
        "\n",
        "    cls = l[4]\n",
        "\n",
        "    x = (xmin + xmax) / 2 / image_w\n",
        "    y = (ymin + ymax) / 2 / image_h\n",
        "    w = (xmax - xmin) / image_w\n",
        "    h = (ymax - ymin) / image_h\n",
        "\n",
        "    loc = [7 * x, 7 * y]\n",
        "    loc_i = int(loc[1])\n",
        "    loc_j = int(loc[0])\n",
        "\n",
        "    y = loc[1] - loc_i\n",
        "    x = loc[0] - loc_j\n",
        "\n",
        "    if label_matrix[loc_i, loc_j, 24] == 0:\n",
        "      label_matrix[loc_i, loc_j, cls] = 1\n",
        "      label_matrix[loc_i, loc_j, 20:24] = [x, y, w, h]\n",
        "      label_matrix[loc_i, loc_j, 24] = 1  # response\n",
        "      \n",
        "  return image, label_matrix"
      ],
      "metadata": {
        "id": "hgMEupdOR7Yd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining custom generator"
      ],
      "metadata": {
        "id": "99vU3S1NT6YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I am defining a custom generator that returns a batch of input and outputs."
      ],
      "metadata": {
        "id": "QalU9mC-T7Ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "j_Owfhz9UQzP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}