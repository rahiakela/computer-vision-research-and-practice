{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "implementing-YOLOV1-from-scratch-using-Keras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN7HCnoeiSRQqL4w2+Poi4A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/computer-vision-research-and-practice/blob/main/computer-vision-case-studies/object-detections/yolo-implementations/yolo-v1/implementing_YOLOV1_from_scratch_using_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementing YOLOV1 from scratch using Keras"
      ],
      "metadata": {
        "id": "dSO4Z9LxSJ5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook I am going to implement YOLOV1 as described in the paper [You Only Look Once](https://arxiv.org/abs/1506.02640). The goal is to replicate the model as described in the paper and in the process, understand the nuances of using Keras on a complex problem.\n",
        "\n",
        "<img src='https://www.maskaravivek.com/post/yolov1/featured_hu2959f475cef1ef9098f72ca1a1294bd8_186245_720x0_resize_lanczos_2.png?raw=1' width='800'/>\n",
        "\n",
        "**Reference**\n",
        "\n",
        "[Implementing YOLOV1 from scratch using Keras Tensorflow 2.0](https://www.maskaravivek.com/post/yolov1/)"
      ],
      "metadata": {
        "id": "Q6OvGZNcSSLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "ZEE1vBosSZgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow import keras\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, InputLayer, Dropout, Flatten, Reshape\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "import argparse\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt    # for plotting the images\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "c8PRQdAYSaut"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would be using [VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/) dataset as its size is manageable so it would be easy to run it using Google Colab.\n",
        "\n",
        "First, I download and extract the dataset."
      ],
      "metadata": {
        "id": "2YtkhLOAWN1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
        "\n",
        "!tar xvf VOCtrainval_06-Nov-2007.tar\n",
        "!tar xvf VOCtest_06-Nov-2007.tar\n",
        "\n",
        "!rm VOCtrainval_06-Nov-2007.tar\n",
        "!rm VOCtest_06-Nov-2007.tar"
      ],
      "metadata": {
        "id": "-7YsLUXKWZ4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preprocessing"
      ],
      "metadata": {
        "id": "bGVvdeQUWgSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we process the annotations and write the labels in a text file. A text file is easier to consume as compared to XML."
      ],
      "metadata": {
        "id": "yyUDdTjeWgup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1yG292gHWne2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}