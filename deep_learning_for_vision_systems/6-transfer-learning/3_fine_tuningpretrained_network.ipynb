{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-fine-tuningpretrained-network.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNGv6ZSIzXalPfVzn4tGtz/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep_learning_for_vision_systems/blob/master/6-transfer-learning/3_fine_tuningpretrained_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMpv3TSD3uRH"
      },
      "source": [
        "## Transfer learning approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMG5KpwN35r_"
      },
      "source": [
        "There are three major transfer learning approaches as follows:\n",
        "\n",
        "1. Pretrained network as a classifier\n",
        "2. Pretrained network as feature extractor\n",
        "3. Fine tuning\n",
        "\n",
        "Each approach can be effective and save significant time in developing and training a deep convolutional neural network model. It may not be clear as to which usage of the pre-trained model may yield the best results on your new computer vision task, therefore some experimentation may be required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YySxJqGD4IxX"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgdfWvDu4MxF"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.applications import mobilenet, imagenet_utils\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.metrics import CategoricalCrossentropy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_files\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RG6oBhc4N8T"
      },
      "source": [
        "## Fine tuning the pretrained network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dGNlsjA4Ovi"
      },
      "source": [
        "we are going to explore scenario #3 that was discussed earlier in this chapter. Where the target dataset is small and it very different from the source dataset. The goal of\n",
        "this project is to build a **Sign Language Classifier** that classifies between 10 classes. The classes are the digits from 0 to 9.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep_learning_for_vision_systems/sign-language-dataset.png?raw=1' width='800'/>\n",
        "\n",
        "**Dataset details**:\n",
        "\n",
        "Our dataset contains the following:\n",
        "\n",
        "- Number of classes = 10 (digits 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9)\n",
        "- Total number of images\n",
        "- Image size = 100x100\n",
        "- Color space = RGB\n",
        "- 1712 images in the training set\n",
        "- 300 images in the validation set\n",
        "- 50 images in the test set\n",
        "\n",
        "\n",
        "It is very noticeable how small our dataset is. If you try to train your network from scratch on\n",
        "this very small dataset, you will not achieve any good results. On the other hand, we were\n",
        "able to achieve an accuracy higher than 98% using transfer learning even though the source\n",
        "and target domains were so different.\n",
        "\n",
        "For ease of comparison with the previous project, we are going to use VGG16 network that\n",
        "was trained on ImageNet dataset as well.\n",
        "\n",
        "The process that we are going to use to fine tune a pretrained network is as follows:\n",
        "\n",
        "1. Preprocess the data to make it ready for the neural network\n",
        "2. Load in pretrained weights from the VGG16 network trained on a large dataset (ImageNet)\n",
        "3. Freeze part of the feature extractor part\n",
        "4. Add the new classifier layers\n",
        "5. Compile the network and run the training process to optimize the model for the smaller dataset\n",
        "6. Evaluate the model\n",
        "\n",
        "Now let’s go through these steps one-by-one and implement this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz2bkMrO6OAX"
      },
      "source": [
        "## 1- Preprocess the data to make it ready for the neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il31CNuPPfxn"
      },
      "source": [
        "Keras has this ImageDataGenerator class which allows us to perform image augmentation on the fly in a very easy way. You can read about that in Keras’s official [documentation](https://keras.io/api/preprocessing/image/). In this example, we are going to use the ImageDataGenerator class to generate our image tensors but we are not going to implement image augmentation for simplicity.\n",
        "\n",
        "The ImageDataGenerator class has a method called flow_from_directory() that is used to read the images from folders containing images. This method expects your data directory to be structured as follows:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep_learning_for_vision_systems/directory-structure.png?raw=1' width='800'/>\n",
        "\n",
        "I have the data structured for you in the Github repo to be ready for you to use\n",
        "flow_from_directory() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnFrq1oE6AMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bed07ddd-c5ab-4ea9-81f6-5501373236e1"
      },
      "source": [
        "# download dataset\n",
        "! git clone https://github.com/rahiakela/machine-learning-datasets -b sign_language_dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'machine-learning-datasets'...\n",
            "remote: Enumerating objects: 4119, done.\u001b[K\n",
            "remote: Counting objects: 100% (4119/4119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3465/3465), done.\u001b[K\n",
            "remote: Total 4119 (delta 655), reused 4117 (delta 653), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (4119/4119), 51.90 MiB | 12.72 MiB/s, done.\n",
            "Resolving deltas: 100% (655/655), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWklkUXAWg1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c0ee5c-c4f9-4006-bd0f-74f3b1afb476"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "machine-learning-datasets  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBHl95m8WxE-"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "# Unzipping files\n",
        "zip_file = \"machine-learning-datasets/sign_language_dataset.zip\"\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9RMN6Ax6hgx"
      },
      "source": [
        "Now, let’s load the data into train_path, valid_path, and\n",
        "test_path variables then generate the train, valid, and test batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX_MPeVM6iIP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775cc7f0-e1d1-4d07-8147-cf574ebcbbeb"
      },
      "source": [
        "train_path  = 'sign_language_dataset/train'\n",
        "valid_path  = 'sign_language_dataset/valid'\n",
        "test_path  = 'sign_language_dataset/test'\n",
        "\n",
        "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation.\n",
        "# The data will be looped over (in batches). in this example, we won't be doing any image augmentation\n",
        "train_batches = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(train_path, target_size=(224, 224), batch_size=10)\n",
        "valid_batches = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(valid_path, target_size=(224, 224), batch_size=30)\n",
        "test_batches  = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(test_path, target_size=(224, 224), batch_size=50, shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1712 images belonging to 10 classes.\n",
            "Found 300 images belonging to 10 classes.\n",
            "Found 50 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykaSCoul7hX7"
      },
      "source": [
        "## 2- Download VGGNet and create network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhoBcr0fdKZK"
      },
      "source": [
        "We are going to download the VGG16 network from Keras and download it’s weights after being pretrained on ImageNet dataset. Remember that we want to remove the classifier part from this network so we will set the parameter include_top=False.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep_learning_for_vision_systems/transfer_network.png?raw=1' width='800'/>\n",
        "\n",
        "Note that we used the parameter pooling=’avg’ here. This is basically means that global average pooling will be applied to the output of the last convolutional layer, and thus the output of the model will be a 2D tensor. We use this as an alternative to the Flatten layer before adding the FC layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xM1gORw7h7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7b848b-086e-4833-eade-a934e6a39b86"
      },
      "source": [
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
        "base_model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL40vydf8ZEd"
      },
      "source": [
        "## 3- Freeze part of the feature extractor part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB7K6UFJeR6f"
      },
      "source": [
        "Here we want to freeze only a part of the feature extraction part and fine tune the rest on our\n",
        "new training data. Remember that in fine tuning we want to freeze part of the network and\n",
        "tune the rest. How we decide the level of fine tuning is usually determined by trial and error.\n",
        "\n",
        "VGG16 has 13 convolutional layers, you can freeze them all or freeze few of them depends on\n",
        "how similar your data is to the source data. In the sign language case, the new domain is very\n",
        "different from our domain, so we will start with fine tuning only the last 5 layers then if we\n",
        "don’t get satisfying results we can fine tune more. It turns out that after we trained the new\n",
        "model, we got 98% accuracy so this is a good level of fine tuning. But in other cases, if you\n",
        "find that your network didn’t converge, try to fine tune more layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPoKD98m8Nvs"
      },
      "source": [
        "# iterate through its layers and lock them except for the last 5 layers\n",
        "for layer in base_model.layers[:-5]:\n",
        "  layer.trainable = False"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lpXCBGie8y5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "080cfc7d-d553-44aa-d431-3903e2a11131"
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 7,079,424\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuDYmu8JfVTz"
      },
      "source": [
        "## 4- Add the new classifier layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZfjbI8jfXDE"
      },
      "source": [
        "Now let's add a few layers on top of the base model. In this example, we will add one FC layer with 64 hidden units and a softmax with 2 hidden units. We will also add batch norm and dropout layers to avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDGWWpTNfDZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "559d25ff-023e-4636-d77e-fd3470a45302"
      },
      "source": [
        "# use “get_layer” method to save the last layer of the network\n",
        "# save the output of the last layer to be the input of the next layer\n",
        "last_layer = base_model.get_layer('global_average_pooling2d')\n",
        "last_output = last_layer.output\n",
        "\n",
        "# add our new softmax layer with 3 hidden units\n",
        "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
        "\n",
        "# instantiate a new_model using keras’s Model class\n",
        "new_model = Model(inputs=base_model.input, outputs=x)\n",
        "new_model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "softmax (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 14,719,818\n",
            "Trainable params: 7,084,554\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MrBnZn8-IvT"
      },
      "source": [
        "## 5- Compile the network and run the training process to optimize the model for the smaller dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZN6oRsPTB2P"
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3GqfqUo9Pbr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cbca01e-fbf3-471b-d0d4-68710c56393d"
      },
      "source": [
        "new_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = new_model.fit(train_batches, steps_per_epoch=18, validation_data=valid_batches, \n",
        "                        validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "18/18 [==============================] - 47s 765ms/step - loss: 2.8163 - accuracy: 0.2667 - val_loss: 1.7561 - val_accuracy: 0.3778\n",
            "Epoch 2/20\n",
            "18/18 [==============================] - 3s 193ms/step - loss: 1.5864 - accuracy: 0.3833 - val_loss: 1.1443 - val_accuracy: 0.6444\n",
            "Epoch 3/20\n",
            "18/18 [==============================] - 3s 192ms/step - loss: 0.8986 - accuracy: 0.6667 - val_loss: 0.5692 - val_accuracy: 0.8111\n",
            "Epoch 4/20\n",
            "18/18 [==============================] - 5s 268ms/step - loss: 0.7285 - accuracy: 0.7674 - val_loss: 0.7601 - val_accuracy: 0.7222\n",
            "Epoch 5/20\n",
            "18/18 [==============================] - 3s 188ms/step - loss: 0.2953 - accuracy: 0.9111 - val_loss: 0.4198 - val_accuracy: 0.8444\n",
            "Epoch 6/20\n",
            "18/18 [==============================] - 3s 187ms/step - loss: 0.1998 - accuracy: 0.9389 - val_loss: 0.2309 - val_accuracy: 0.9000\n",
            "Epoch 7/20\n",
            "18/18 [==============================] - 3s 188ms/step - loss: 0.1217 - accuracy: 0.9611 - val_loss: 0.1363 - val_accuracy: 0.9444\n",
            "Epoch 8/20\n",
            "18/18 [==============================] - 3s 188ms/step - loss: 0.1517 - accuracy: 0.9667 - val_loss: 0.2124 - val_accuracy: 0.9556\n",
            "Epoch 9/20\n",
            "18/18 [==============================] - 3s 187ms/step - loss: 0.1255 - accuracy: 0.9500 - val_loss: 0.1106 - val_accuracy: 0.9778\n",
            "Epoch 10/20\n",
            "18/18 [==============================] - 3s 192ms/step - loss: 0.1299 - accuracy: 0.9556 - val_loss: 0.1504 - val_accuracy: 0.9444\n",
            "Epoch 11/20\n",
            "18/18 [==============================] - 3s 188ms/step - loss: 0.0595 - accuracy: 0.9833 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "18/18 [==============================] - 3s 189ms/step - loss: 0.0939 - accuracy: 0.9667 - val_loss: 0.3639 - val_accuracy: 0.8778\n",
            "Epoch 13/20\n",
            "18/18 [==============================] - 3s 188ms/step - loss: 0.0923 - accuracy: 0.9722 - val_loss: 0.2107 - val_accuracy: 0.9556\n",
            "Epoch 14/20\n",
            "18/18 [==============================] - 3s 188ms/step - loss: 0.0392 - accuracy: 0.9944 - val_loss: 0.2741 - val_accuracy: 0.9556\n",
            "Epoch 15/20\n",
            "18/18 [==============================] - 3s 185ms/step - loss: 0.0554 - accuracy: 0.9767 - val_loss: 0.1282 - val_accuracy: 0.9667\n",
            "Epoch 16/20\n",
            "18/18 [==============================] - 4s 197ms/step - loss: 0.1409 - accuracy: 0.9444 - val_loss: 0.1255 - val_accuracy: 0.9667\n",
            "Epoch 17/20\n",
            "18/18 [==============================] - 3s 193ms/step - loss: 0.0717 - accuracy: 0.9833 - val_loss: 0.0293 - val_accuracy: 0.9889\n",
            "Epoch 18/20\n",
            "18/18 [==============================] - 4s 188ms/step - loss: 0.0234 - accuracy: 0.9944 - val_loss: 0.0670 - val_accuracy: 0.9667\n",
            "Epoch 19/20\n",
            "18/18 [==============================] - 3s 189ms/step - loss: 0.0208 - accuracy: 0.9889 - val_loss: 0.0678 - val_accuracy: 0.9778\n",
            "Epoch 20/20\n",
            "18/18 [==============================] - 3s 187ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2610 - val_accuracy: 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG1zPlvu_zFu"
      },
      "source": [
        "Notice the training time of each epoch from the verbose above. It took the model approximately the model was trained very quickly using a regular CPU computing power. Each\n",
        "epoch took approximately 25 to 29 seconds which means that it took the model less than 10 minutes to train for 20 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAs9NVS-_4At"
      },
      "source": [
        "## 6- Evaluate the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFY73jeOABGd"
      },
      "source": [
        "Now, let’s use Keras’s evaluate() method to calculate the model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Op3n7lS-4FZ"
      },
      "source": [
        "def load_dataset(path):\n",
        "  data = load_files(path)\n",
        "  paths = np.array(data['filenames'])\n",
        "  targets = to_categorical(np.array(data['target']))\n",
        "\n",
        "  return paths, targets"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPiRVT2LBM7h"
      },
      "source": [
        "test_files, test_targets = load_dataset(\"sign_language_dataset/test\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9L1H9DmC9i5"
      },
      "source": [
        "def path_to_tensor(img_path): \n",
        "  # loads RGB image as PIL.Image.Image type\n",
        "  image = load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "  # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
        "  x = img_to_array(image)\n",
        "\n",
        "  # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor \n",
        "  return np.expand_dims(x, axis=0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iDHKORNEHOU"
      },
      "source": [
        "def paths_to_tensor(img_paths):\n",
        "  list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
        "\n",
        "  return np.vstack(list_of_tensors)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s6lhC8UExhA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b107f44-4a5f-4670-8548-48275cb6c2a9"
      },
      "source": [
        "test_tensors = preprocess_input(paths_to_tensor(test_files))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 519.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGn1RuXGUeoz"
      },
      "source": [
        "Loading the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yMmnMBuUiXZ"
      },
      "source": [
        "new_model.load_weights('signlanguage.model.hdf5')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfCugfV2FE7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fadc25f-dc98-4aeb-dace-375e4322cb86"
      },
      "source": [
        "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 17s 6s/step - loss: 0.0422 - accuracy: 0.9800\n",
            "\n",
            "Testing loss: 0.0422\n",
            "Testing accuracy: 0.9800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEP13uUVUsvk"
      },
      "source": [
        "A deeper level of evaluating your model is to create the confusion matrix. It is a table that is often used to describe the performance of a classification model to give us a deeper understanding of how the model performed on the test dataset.\n",
        "\n",
        "Now, let’s build the confusion matrix for our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krLu8Cy-Ur08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "c9684952-6a2e-43a0-d9b3-c20cad3bb9da"
      },
      "source": [
        "cm_labels = ['0','1','2','3','4','5','6','7','8','9']\n",
        "\n",
        "cm = confusion_matrix(np.argmax(test_targets, axis=1), np.argmax(new_model.predict(test_tensors), axis=1))\n",
        "plt.imshow(cm, cmap=plt.cm.Blues)\n",
        "plt.colorbar()\n",
        "indexes = np.arange(len(cm_labels))\n",
        "for i in indexes:\n",
        "    for j in indexes:\n",
        "        plt.text(j, i, cm[i, j])\n",
        "plt.xticks(indexes, cm_labels, rotation=90)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.yticks(indexes, cm_labels)\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion matrix')\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6svznYJqVHun"
      },
      "source": [
        "To read this confusion matrix, look at the number in the Predicted Label axis and check if it\n",
        "was correctly classified in the True Label axis. For example, look at number 0 in predicted\n",
        "label axis, you will see that all 5 images are classified as 0 and zero number of images were\n",
        "mistakenly classified as any other number. Similarly, go through the rest of the numbers in\n",
        "the predicted label axis. You will notice that the model as successfully made the correct\n",
        "predictions for all the test images except for the image with true label = 8. Where the model\n",
        "has mistakenly classified an image of number 8 and thought it was number = 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss9hdlV6YMul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c086d3c-2f66-40d8-e184-1336d261ad8f"
      },
      "source": [
        "# evaluate and print test accuracy\n",
        "score = new_model.evaluate(test_tensors, test_targets)\n",
        "print('\\n', 'Test accuracy:', score[1])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 213ms/step - loss: 0.0422 - accuracy: 0.9800\n",
            "\n",
            " Test accuracy: 0.9800000190734863\n"
          ]
        }
      ]
    }
  ]
}