{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-pretrained-network-as-feature-extractor.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMOy0SM8Eku81Yb5O3ntagS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep_learning_for_vision_systems/blob/master/6-transfer-learning/2_pretrained_network_as_feature_extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMpv3TSD3uRH"
      },
      "source": [
        "##Project 1: A pretrained network as a feature extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMG5KpwN35r_"
      },
      "source": [
        "In this project, we use a very small amount of data to train a classifier that detects images of dogs and cats. This is a pretty simple project, but the goal of the exercise is to see how to implement transfer learning when you have a very small amount of data and the target domain is similar to the source domain.\n",
        "\n",
        "We will use the pretrained convolutional network as a feature\n",
        "extractor. This means we are going to freeze the feature extractor part of the network, add our own classifier, and then retrain the network on our new small dataset.\n",
        "\n",
        "For this implementation, we’ll be using the VGG16. Although it didn’t record the\n",
        "lowest error in the ILSVRC, I found that it worked well for the task and was quicker to train than other models. I got an accuracy of about 96%, but you can feel free to use GoogLeNet or ResNet to experiment and compare results.\n",
        "\n",
        "There are three major transfer learning approaches as follows:\n",
        "\n",
        "1. Pretrained network as a classifier\n",
        "2. Pretrained network as feature extractor\n",
        "3. Fine tuning\n",
        "\n",
        "Each approach can be effective and save significant time in developing and training a deep convolutional neural network model. It may not be clear as to which usage of the pre-trained model may yield the best results on your new computer vision task, therefore some experimentation may be required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YySxJqGD4IxX"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgdfWvDu4MxF"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.applications import mobilenet, imagenet_utils\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.metrics import CategoricalCrossentropy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_files\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RG6oBhc4N8T"
      },
      "source": [
        "## Pretrained network as a feature extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dGNlsjA4Ovi"
      },
      "source": [
        "One other important takeaway from this project is to learn how to preprocess custom data and make it ready to train your neural network. In previous projects, we used CIFAR and MNIST datasets which are already preprocessed for us by Keras and all we had to do is to download it from Keras library and directly use them to train the network. In this project, I’m going to show a tutorial on how to structure your data repository and use Keras library to get your data ready.\n",
        "\n",
        "The process to use a pretrained model as a feature extractor is well-established:\n",
        "\n",
        "1. Preprocess the data to make it ready for the neural network.\n",
        "2. Load in pretrained weights from the VGG16 network trained on a large dataset.\n",
        "3. Freeze all the weights in the convolutional layers (feature extraction part). Remember - the layers to freeze are adjusted depending on the similarity of new task to original dataset. In our case, we observed that ImageNet has a lot of dogs and cats images so the network has already been trained to extract the detailed features of our target object.\n",
        "\n",
        "4. Replace the fully-connected layers of the network with a custom classifier. You can add as many FC layers as you see fit and each have as many hidden units as you want. For simple problem like this, we are just going to add one hidden layer with 64 units. You can observe the results and tune up if the model is underfitting or down if the model is overfitting. For the softmax layer, the number of units must be set equal to the number of classes (2 units in our case).\n",
        "5. Compile the network and run the training process on the new data of cats and dogs to optimize the model for the smaller dataset.\n",
        "6. Evaluate the model.\n",
        "\n",
        "Now let’s go through these steps one-by-one and implement this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz2bkMrO6OAX"
      },
      "source": [
        "## 1- Preprocess the data to make it ready for the neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il31CNuPPfxn"
      },
      "source": [
        "Keras has this ImageDataGenerator class which allows us to perform image augmentation on the fly in a very easy way. You can read about that in Keras’s official [documentation](https://keras.io/api/preprocessing/image/). In this example, we are going to use the ImageDataGenerator class to generate our image tensors but we are not going to implement image augmentation for simplicity.\n",
        "\n",
        "The ImageDataGenerator class has a method called flow_from_directory() that is used to read the images from folders containing images. This method expects your data directory to be structured as follows:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep_learning_for_vision_systems/directory-structure.png?raw=1' width='800'/>\n",
        "\n",
        "I have the data structured for you in the Github repo to be ready for you to use\n",
        "flow_from_directory() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnFrq1oE6AMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba475f10-a33f-44cb-a415-b6019fb5dfac"
      },
      "source": [
        "# download dataset\n",
        "!git clone https://github.com/rahiakela/machine-learning-datasets -b dogs_vs_cats_dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'machine-learning-datasets'...\n",
            "remote: Enumerating objects: 4119, done.\u001b[K\n",
            "remote: Counting objects: 100% (4119/4119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3465/3465), done.\u001b[K\n",
            "remote: Total 4119 (delta 655), reused 4117 (delta 653), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (4119/4119), 51.90 MiB | 29.52 MiB/s, done.\n",
            "Resolving deltas: 100% (655/655), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWklkUXAWg1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d15ebfcb-b347-4eef-b705-47f55f348f22"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "machine-learning-datasets  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBHl95m8WxE-"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "# Unzipping files\n",
        "zip_file = \"machine-learning-datasets/dogs_vs_cats_dataset.zip\"\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9RMN6Ax6hgx"
      },
      "source": [
        "Now, let’s load the data into train_path, valid_path, and\n",
        "test_path variables then generate the train, valid, and test batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX_MPeVM6iIP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af990699-2c79-446d-d813-f305b247100a"
      },
      "source": [
        "train_path  = 'dogs_vs_cats_dataset/data/train'\n",
        "valid_path  = 'dogs_vs_cats_dataset/data/valid'\n",
        "test_path  = 'dogs_vs_cats_dataset/data/test'\n",
        "\n",
        "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation.\n",
        "# The data will be looped over (in batches). in this example, we won't be doing any image augmentation\n",
        "train_batches = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(train_path, target_size=(224, 224), batch_size=10)\n",
        "valid_batches = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(valid_path, target_size=(224, 224), batch_size=30)\n",
        "test_batches  = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory(test_path, target_size=(224, 224), batch_size=50, shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 202 images belonging to 2 classes.\n",
            "Found 103 images belonging to 2 classes.\n",
            "Found 451 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykaSCoul7hX7"
      },
      "source": [
        "## 2- Download VGGNet and create network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhoBcr0fdKZK"
      },
      "source": [
        "We are going to download the VGG16 network from Keras and download it’s weights after being pretrained on ImageNet dataset. Remember that we want to remove the classifier part from this network so we will set the parameter include_top=False.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep_learning_for_vision_systems/transfer_network.png?raw=1' width='800'/>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xM1gORw7h7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f82005-08a7-4efb-83e5-0329b29ecdd2"
      },
      "source": [
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL40vydf8ZEd"
      },
      "source": [
        "## 3- Freeze all the weights in the convolutional layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB7K6UFJeR6f"
      },
      "source": [
        "We will freeze the convolutional layers from the base_model created from the previous step and use that as a feature extractor, then add a classifier on top of it in the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPoKD98m8Nvs"
      },
      "source": [
        "# iterate through its layers and lock them to make them not trainable\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lpXCBGie8y5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8bb7f2b-c5dc-4b47-c5b8-97de8cd7bca6"
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuDYmu8JfVTz"
      },
      "source": [
        "## 4- Replace the fully-connected layers of the network with a custom classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZfjbI8jfXDE"
      },
      "source": [
        "Now let's add a few layers on top of the base model. In this example, we will add one FC layer with 64 hidden units and a softmax with 2 hidden units. We will also add batch norm and dropout layers to avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDGWWpTNfDZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f59bc08-5346-40ec-a7b7-41c0f55a9f16"
      },
      "source": [
        "# use “get_layer” method to save the last layer of the network\n",
        "# save the output of the last layer to be the input of the next layer\n",
        "last_layer = base_model.get_layer('block5_pool')\n",
        "last_output = last_layer.output\n",
        "\n",
        "# flatten the classifier input which is output of the last layer of VGG16 model\n",
        "x = Flatten()(last_output)\n",
        "\n",
        "# add 1 FC layers that has 64 units, batchnorm, dropout, and softmax layers\n",
        "x = Dense(64, activation='relu', name='FC_2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "# add our new softmax layer with 3 hidden units\n",
        "x = Dense(2, activation='softmax', name='softmax')(x)\n",
        "\n",
        "# instantiate a new_model using keras’s Model class\n",
        "new_model = Model(inputs=base_model.input, outputs=x)\n",
        "new_model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "FC_2 (Dense)                 (None, 64)                1605696   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "softmax (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 16,320,770\n",
            "Trainable params: 1,605,954\n",
            "Non-trainable params: 14,714,816\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MrBnZn8-IvT"
      },
      "source": [
        "## 5- Compile the network and run the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3GqfqUo9Pbr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d93688f2-04bd-4d36-b0cb-bcc4f48e72bd"
      },
      "source": [
        "new_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "new_model.fit_generator(train_batches, steps_per_epoch=4, validation_data=valid_batches, validation_steps=2, epochs=20, verbose=2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "4/4 - 63s - loss: 1.1611 - accuracy: 0.4500 - val_loss: 3.6035 - val_accuracy: 0.5167\n",
            "Epoch 2/20\n",
            "4/4 - 51s - loss: 1.1480 - accuracy: 0.6250 - val_loss: 1.4930 - val_accuracy: 0.7000\n",
            "Epoch 3/20\n",
            "4/4 - 50s - loss: 0.5984 - accuracy: 0.7000 - val_loss: 0.7127 - val_accuracy: 0.8000\n",
            "Epoch 4/20\n",
            "4/4 - 50s - loss: 0.3281 - accuracy: 0.8250 - val_loss: 0.7115 - val_accuracy: 0.8167\n",
            "Epoch 5/20\n",
            "4/4 - 50s - loss: 0.5290 - accuracy: 0.7750 - val_loss: 0.3947 - val_accuracy: 0.8833\n",
            "Epoch 6/20\n",
            "4/4 - 47s - loss: 0.2074 - accuracy: 0.9375 - val_loss: 0.2994 - val_accuracy: 0.9000\n",
            "Epoch 7/20\n",
            "4/4 - 50s - loss: 0.1847 - accuracy: 0.9250 - val_loss: 0.1828 - val_accuracy: 0.9333\n",
            "Epoch 8/20\n",
            "4/4 - 50s - loss: 0.3079 - accuracy: 0.8500 - val_loss: 0.2062 - val_accuracy: 0.9333\n",
            "Epoch 9/20\n",
            "4/4 - 50s - loss: 0.2192 - accuracy: 0.9000 - val_loss: 0.3646 - val_accuracy: 0.8500\n",
            "Epoch 10/20\n",
            "4/4 - 61s - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.3331 - val_accuracy: 0.8833\n",
            "Epoch 11/20\n",
            "4/4 - 50s - loss: 0.1142 - accuracy: 0.9500 - val_loss: 0.2484 - val_accuracy: 0.9333\n",
            "Epoch 12/20\n",
            "4/4 - 50s - loss: 0.2607 - accuracy: 0.8750 - val_loss: 0.2241 - val_accuracy: 0.9167\n",
            "Epoch 13/20\n",
            "4/4 - 46s - loss: 0.2037 - accuracy: 0.8750 - val_loss: 0.1262 - val_accuracy: 0.9333\n",
            "Epoch 14/20\n",
            "4/4 - 50s - loss: 0.1631 - accuracy: 0.9250 - val_loss: 0.2520 - val_accuracy: 0.9000\n",
            "Epoch 15/20\n",
            "4/4 - 61s - loss: 0.0421 - accuracy: 1.0000 - val_loss: 0.1620 - val_accuracy: 0.9167\n",
            "Epoch 16/20\n",
            "4/4 - 46s - loss: 0.0413 - accuracy: 1.0000 - val_loss: 0.1804 - val_accuracy: 0.8833\n",
            "Epoch 17/20\n",
            "4/4 - 50s - loss: 0.0968 - accuracy: 0.9750 - val_loss: 0.1659 - val_accuracy: 0.9167\n",
            "Epoch 18/20\n",
            "4/4 - 50s - loss: 0.3982 - accuracy: 0.9000 - val_loss: 0.1349 - val_accuracy: 0.9333\n",
            "Epoch 19/20\n",
            "4/4 - 50s - loss: 0.1611 - accuracy: 0.9500 - val_loss: 0.1556 - val_accuracy: 0.9500\n",
            "Epoch 20/20\n",
            "4/4 - 50s - loss: 0.1081 - accuracy: 0.9500 - val_loss: 0.2458 - val_accuracy: 0.8667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd4cac937d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG1zPlvu_zFu"
      },
      "source": [
        "Notice that the model was trained very quickly using a regular CPU computing power. Each epoch took approximately 25 to 29 seconds which means that it took the model less than 10 minutes to train for 20 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAs9NVS-_4At"
      },
      "source": [
        "## 6- Evaluate the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFY73jeOABGd"
      },
      "source": [
        "Now, let’s use Keras’s evaluate() method to calculate the model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Op3n7lS-4FZ"
      },
      "source": [
        "def load_dataset(path):\n",
        "  data = load_files(path)\n",
        "  paths = np.array(data['filenames'])\n",
        "  targets = to_categorical(np.array(data['target']))\n",
        "\n",
        "  return paths, targets"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPiRVT2LBM7h"
      },
      "source": [
        "test_files, test_targets = load_dataset(\"dogs_vs_cats_dataset/data/test\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tha0-c2Ei5ht"
      },
      "source": [
        "Then, we create test_tensors to evaluate the model on them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9L1H9DmC9i5"
      },
      "source": [
        "def path_to_tensor(img_path): \n",
        "  # loads RGB image as PIL.Image.Image type\n",
        "  img = load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "  # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
        "  x = img_to_array(img)\n",
        "\n",
        "  # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor \n",
        "  return np.expand_dims(x, axis=0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iDHKORNEHOU"
      },
      "source": [
        "def paths_to_tensor(img_paths):\n",
        "  list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
        "\n",
        "  return np.vstack(list_of_tensors)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s6lhC8UExhA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e3991e-3763-47f8-d31e-f6a6caa2f326"
      },
      "source": [
        "test_tensors = preprocess_input(paths_to_tensor(test_files))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 451/451 [00:03<00:00, 137.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbFanrUpjYWI"
      },
      "source": [
        "Now we can run Keras’s evaluate() method to calculate the model accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfCugfV2FE7I",
        "outputId": "599c466c-0273-47dc-f465-87d140b82f54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 222s 15s/step - loss: 0.1069 - accuracy: 0.9512\n",
            "\n",
            "Testing loss: 0.1069\n",
            "Testing accuracy: 0.9512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss9hdlV6YMul",
        "outputId": "641a8ff4-e5b8-4fdb-f1b4-a51b32695929",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# evaluate and print test accuracy\n",
        "score = new_model.evaluate(test_tensors, test_targets)\n",
        "print('\\n', 'Test accuracy:', score[1])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 223s 15s/step - loss: 0.1069 - accuracy: 0.9512\n",
            "\n",
            " Test accuracy: 0.9512194991111755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmbaubqPfmV_"
      },
      "source": [
        "The model has achieved an accuracy of 95.12% in less than 10 minutes of training.\n",
        "\n",
        "This is very good, given our very small dataset."
      ]
    }
  ]
}