{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-resnet-networks.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdxgsdWHlYuhOoyecdzR4q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/3-convolutional-and-residual-neural-networks/3_resnet_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlFMNBrQMx4o"
      },
      "source": [
        "##ResNet networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QclXMBouMyyh"
      },
      "source": [
        "The researchers for the residual block design pattern component of the residual\n",
        "network proposed a new novel layer connection they called an identity link. \n",
        "\n",
        "The identity\n",
        "link introduced the earliest concept of feature reuse. Prior to the identity link,\n",
        "each convolutional block did feature extraction on the previous convolutional output,\n",
        "without retaining any knowledge from prior outputs. The identity link can be seen as\n",
        "a coupling between the current and prior convolutional outputs to reuse feature\n",
        "information gained from earlier extraction.\n",
        "\n",
        "Using identity links along with batch normalization provided more stability across\n",
        "layers, reducing both vanishing and exploding gradients and divergence between\n",
        "layers, allowing model architectures to go deeper in layers to increase accuracy in\n",
        "prediction.\n",
        "\n",
        "ResNet34 introduced a new block layer and layer-connection pattern, residual\n",
        "blocks, and identity connection, respectively. The residual block in ResNet34 consists\n",
        "of blocks of two identical convolutional layers without a pooling layer. Each block has\n",
        "an identity connection that creates a parallel path between the input of the residual\n",
        "block and its output.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/3-convolutional-and-residual-neural-networks/images/2.png?raw=1' width='800'/>\n",
        "\n",
        "One of the problems with neural networks is that as we add deeper layers (under the\n",
        "presumption of increasing accuracy), their performance can degrade. It can get\n",
        "worse, not better. This occurs for several reasons. As we go deeper, we are adding\n",
        "more parameters (weights). The more parameters, the more places that each input in\n",
        "the training data will fit to the excess parameters. Instead of generalizing, the neural\n",
        "network will simply learn each training example (rote memorization).\n",
        "\n",
        "The other issue\n",
        "is covariate shift: the distribution of the weights will widen (spread further apart) as we\n",
        "go deeper, resulting in making it more difficult for the neural network to converge.\n",
        "The former case causes a degradation in performance on the test (holdout) data, and\n",
        "the latter, on the training data as well as a vanishing or exploding gradient.\n",
        "\n",
        "Residual blocks allow neural networks to be built with deeper layers without a\n",
        "degradation in performance on the test data. A ResNet block could be viewed as a\n",
        "VGG block with the addition of the identity link.\n",
        "\n",
        "While the VGG style of the block performs\n",
        "feature detection, the identity link retains the input for the next subsequent\n",
        "block, whereby the input to the next block consists of both the previous features’\n",
        "detection and input.\n",
        "\n",
        "By retaining information from the past (previous input), this block design allows\n",
        "neural networks to go deeper than the VGG counterpart, with an increase in accuracy.\n",
        "Mathematically, we could represent the VGG and ResNet as follows.\n",
        "\n",
        "VGG: $h(x)=f(x, {W})$\n",
        "\n",
        "ResNet:  $h(x)=f(x, {W}) + x$\n",
        "\n",
        "The variable x represents the output of a\n",
        "layer, which is the input to the next layer. At the beginning of the block, we retain a\n",
        "copy of the previous block/layer output as the variable shortcut. We then pass the\n",
        "previous block/layer output (x) through two convolutional layers, each time taking\n",
        "the output from the previous layer as input into the next layer. Finally, the last output\n",
        "from the block (retained in the variable x) is added (matrix addition) with the original\n",
        "value of x (shortcut).\n",
        "\n",
        "```python\n",
        "shortcut = x  # Remember the input to the block.\n",
        "x = layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "x = layers.ReLU()(x)\n",
        "x = layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "x = layers.ReLU()(x)   # The output of the convolutional sequence\n",
        "x = layers.add([shortcut, x])  # Matrix addition of the input to the output\n",
        "```\n",
        "\n",
        "The ResNet architectures take as input a (224, 224, 3) vector—an RGB image\n",
        "(3 channels) of 224 (height) × 224 (width) pixels. The first layer is a basic convolutional\n",
        "layer, consisting of a convolution using a fairly large filter size of 7 × 7. The output\n",
        "(feature maps) is then reduced in size by a max pooling layer.\n",
        "\n",
        "After the initial convolutional layer is a succession of groups of residual blocks.\n",
        "Each successive group doubles the number of filters (similar to VGG). Unlike VGG,\n",
        "though, there is no pooling layer between the groups that would reduce the size of\n",
        "the feature maps.\n",
        "\n",
        "The identity link would\n",
        "attempt to add the input matrix (X) and the output matrix (2X). Yikes—we get an\n",
        "error, indicating we can’t broadcast (for the add operation) matrices of different sizes.\n",
        "\n",
        "For ResNet, this is solved by adding a convolutional block between each “doubling”\n",
        "group of residual blocks.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/3-convolutional-and-residual-neural-networks/images/3.png?raw=1' width='800'/>\n",
        "\n",
        "The output of the last residual block group is passed to a pooling and flattening layer\n",
        "(GlobalAveragePooling2D), which is then passed to a single Dense layer of 1000\n",
        "nodes (number of classes).\n",
        "\n",
        "Let’s now put the whole network together, using a procedural style. Additionally, we\n",
        "need to add the entry convolutional layer of ResNet and then the DNN classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLUeNQMoOZni"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW4XoIp4Oay8"
      },
      "source": [
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Dense, ReLU, Activation, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, GlobalAveragePooling2D, ZeroPadding2D"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vokTefxkObFa"
      },
      "source": [
        "##ResNet using procedural style"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZGSfjNpOwCH"
      },
      "source": [
        "def residual_block(n_filters, x):\n",
        "  \"\"\"\n",
        "  Create a Residual Block of Convolutions\n",
        "  n_filters: number of filters\n",
        "  x        : input into the block\n",
        "  \"\"\"\n",
        "  shortcut = x\n",
        "  x = Conv2D(n_filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", activation=\"relu\")(x)\n",
        "  x = Conv2D(n_filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", activation=\"relu\")(x)\n",
        "  x = layers.add([shortcut, x])\n",
        "  return x\n",
        "\n",
        "def conv_block(n_filters, x):\n",
        "  \"\"\"\n",
        "  Create Block of Convolutions without Pooling\n",
        "  n_filters: number of filters\n",
        "  x        : input into the block\n",
        "  \"\"\"\n",
        "  x = Conv2D(n_filters, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\")(x)\n",
        "  x = Conv2D(n_filters, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\")(x)\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zlQqVlMOdmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae41564-9dbb-4418-a39c-948c021e5a44"
      },
      "source": [
        "# The input tensor  \n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "\n",
        "# First convolutional layer, where pooled feature maps will be reduced by 75%\n",
        "x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding=\"same\", activation=\"relu\")(inputs)\n",
        "x = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(x)\n",
        "\n",
        "# First residual block group of 64 filters\n",
        "for _ in range(2):\n",
        "  x = residual_block(64, x)\n",
        "\n",
        "# Doubles the size of filters and reduces feature maps by 75% (stride s = 2, 2) to fit the next residual group\n",
        "x = conv_block(128, x)\n",
        "\n",
        "# Second residual block group of 128 filters\n",
        "for _ in range(3):\n",
        "  x = residual_block(128, x)\n",
        "\n",
        "x = conv_block(256, x)\n",
        "\n",
        "# Third residual block group of 256 filters\n",
        "for _ in range(5):\n",
        "  x = residual_block(256, x)\n",
        "\n",
        "x = conv_block(512, x)\n",
        "\n",
        "# Fourth residual block group of 512 filters\n",
        "x = residual_block(512, x)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Output layer for classification (1000 classes)\n",
        "outputs = Dense(1000, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 112, 112, 64) 9472        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 56, 56, 64)   0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 56, 56, 64)   36928       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 56, 56, 64)   36928       conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 56, 56, 64)   0           max_pooling2d[0][0]              \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 56, 56, 64)   36928       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 56, 56, 64)   36928       conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 56, 56, 64)   0           add[0][0]                        \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 28, 28, 128)  73856       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 14, 14, 128)  147584      conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 14, 14, 128)  147584      conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 14, 14, 128)  147584      conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 14, 14, 128)  0           conv2d_6[0][0]                   \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 14, 14, 128)  147584      add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 14, 14, 128)  147584      conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 14, 14, 128)  0           add_2[0][0]                      \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 14, 14, 128)  147584      add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 14, 14, 128)  147584      conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 14, 14, 128)  0           add_3[0][0]                      \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 7, 7, 256)    295168      add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 4, 4, 256)    0           conv2d_14[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 4, 4, 256)    590080      add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 4, 4, 256)    0           add_5[0][0]                      \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 4, 4, 256)    590080      add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 4, 4, 256)    0           add_6[0][0]                      \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 4, 4, 256)    590080      add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 4, 4, 256)    0           add_7[0][0]                      \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 4, 4, 256)    590080      add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 4, 4, 256)    0           add_8[0][0]                      \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 2, 2, 512)    1180160     add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 1, 1, 512)    2359808     conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 1, 1, 512)    2359808     conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 1, 1, 512)    2359808     conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 1, 1, 512)    0           conv2d_26[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 512)          0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1000)         513000      global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 16,822,760\n",
            "Trainable params: 16,822,760\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMOwJ4phResN"
      },
      "source": [
        "We see that the total number of parameters to learn\n",
        "is 21 million. This is in contrast to the VGG16, which has 138 million parameters. So\n",
        "the ResNet architecture is six times computationally faster. This reduction is mostly\n",
        "achieved by the construction of the residual blocks.\n",
        "\n",
        "Notice that the DNN backend is\n",
        "just a single output Dense layer. In effect, there is no backend. The early residual\n",
        "block groups act as the CNN frontend doing the feature detection, while the latter\n",
        "residual blocks perform the classification. \n",
        "\n",
        "In doing so, unlike in VGG, there was no\n",
        "need for several fully connected dense layers, which would have substantially\n",
        "increased the number of parameters.\n",
        "\n",
        "Unlike the previous example of pooling, in which the size of each feature map is\n",
        "reduced according to the size of the stride, GlobalAveragePooling2D is like a supercharged\n",
        "version of pooling: each feature map is replaced by a single value, which in\n",
        "this case is the average of all values in the corresponding feature map. For example, if\n",
        "the input is 256 feature maps, the output will be a 1D vector of size 256. \n",
        "\n",
        "After ResNet,\n",
        "it became the general practice for deep convolutional neural networks to use Global-\n",
        "AveragePooling2D at the last pooling stage, which benefited from a substantial\n",
        "reduction of the number of parameters coming into the classifier, without significant\n",
        "loss in representational power.\n",
        "\n",
        "Another advantage is the identity link, which provided the ability to add deeper\n",
        "layers, without degradation, for higher accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhaH6Kb0R87O"
      },
      "source": [
        "##ResNet with bottleneck residual block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM-QS5pC-cDw"
      },
      "source": [
        "ResNet50 introduced a variation of the residual block referred to as the bottleneck\n",
        "residual block. In this version, the group of two 3 × 3 convolutional layers is replaced by\n",
        "a group of 1 × 1, then 3 × 3, and then 1 × 1 convolutional layers. \n",
        "\n",
        "The first 1 × 1 convolution\n",
        "performs a dimensionality reduction, reducing the computational complexity,\n",
        "and the last convolution restores the dimensionality, increasing the number of filters\n",
        "by a factor of 4. \n",
        "\n",
        "The middle 3 × 3 convolution is referred to as the bottleneck convolution,\n",
        "like the neck of a bottle. \n",
        "\n",
        "The bottleneck residual block, allows\n",
        "for deeper neural networks, without degradation, and further reduction in computational\n",
        "complexity.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/3-convolutional-and-residual-neural-networks/images/4.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aYs3NyTR_J_"
      },
      "source": [
        "def bottleneck_block(n_filters, x):\n",
        "  \"\"\"\n",
        "  Create a Bottleneck Residual Block of Convolutions\n",
        "  n_filters: number of filters\n",
        "  x        : input into the block\n",
        "  \"\"\"\n",
        "  shortcut = x\n",
        "  # A 1 × 1 bottleneck convolution for dimensionality reduction\n",
        "  x = Conv2D(n_filters, kernel_size=(1, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(x)\n",
        "  # A 3 × 3 convolution for feature extraction\n",
        "  x = Conv2D(n_filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", activation=\"relu\")(x)\n",
        "  # A 1 × 1 projection convolution for dimensionality expansion\n",
        "  x = Conv2D(n_filters * 4, kernel_size=(1, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(x)\n",
        "  # Matrix addition of the input to the output\n",
        "  x = layers.add([shortcut, x])\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb5t4ybEtxvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94467e6e-c72e-4938-ee6d-dc79b555f52b"
      },
      "source": [
        "# The input tensor  \n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "\n",
        "# First convolutional layer, where pooled feature maps will be reduced by 75%\n",
        "x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding=\"same\", activation=\"relu\")(inputs)\n",
        "x = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(x)\n",
        "\n",
        "# First residual block group of 64 filters\n",
        "for _ in range(2):\n",
        "  x = residual_block(64, x)\n",
        "\n",
        "# Doubles the size of filters and reduces feature maps by 75% (stride s = 2, 2) to fit the next residual group\n",
        "x = conv_block(128, x)\n",
        "\n",
        "# Second residual block group of 128 filters\n",
        "for _ in range(3):\n",
        "  x = residual_block(128, x)\n",
        "\n",
        "x = conv_block(256, x)\n",
        "\n",
        "# Third residual block group of 256 filters\n",
        "for _ in range(5):\n",
        "  x = residual_block(256, x)\n",
        "\n",
        "x = conv_block(512, x)\n",
        "\n",
        "# Fourth residual block group of 512 filters\n",
        "x = residual_block(512, x)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Output layer for classification (1000 classes)\n",
        "outputs = Dense(1000, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 112, 112, 64) 9472        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 56, 56, 64)   0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 56, 56, 64)   36928       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 56, 56, 64)   36928       conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 56, 56, 64)   0           max_pooling2d_2[0][0]            \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 56, 56, 64)   36928       add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 56, 56, 64)   36928       conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 56, 56, 64)   0           add_12[0][0]                     \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 28, 28, 128)  73856       add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 14, 14, 128)  147584      conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 14, 14, 128)  147584      conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 14, 14, 128)  147584      conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 14, 14, 128)  0           conv2d_39[0][0]                  \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 14, 14, 128)  147584      add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 14, 14, 128)  147584      conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 14, 14, 128)  0           add_14[0][0]                     \n",
            "                                                                 conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 14, 14, 128)  147584      add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 14, 14, 128)  147584      conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 14, 14, 128)  0           add_15[0][0]                     \n",
            "                                                                 conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 7, 7, 256)    295168      add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 4, 4, 256)    0           conv2d_47[0][0]                  \n",
            "                                                                 conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 4, 4, 256)    590080      add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 4, 4, 256)    0           add_17[0][0]                     \n",
            "                                                                 conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 4, 4, 256)    590080      add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 4, 4, 256)    0           add_18[0][0]                     \n",
            "                                                                 conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 4, 4, 256)    590080      add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 4, 4, 256)    0           add_19[0][0]                     \n",
            "                                                                 conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 4, 4, 256)    590080      add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 4, 4, 256)    0           add_20[0][0]                     \n",
            "                                                                 conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 2, 2, 512)    1180160     add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 1, 1, 512)    2359808     conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 1, 1, 512)    2359808     conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 1, 1, 512)    2359808     conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 1, 1, 512)    0           conv2d_59[0][0]                  \n",
            "                                                                 conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 512)          0           add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1000)         513000      global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 16,822,760\n",
            "Trainable params: 16,822,760\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sEFYuc6vUyi"
      },
      "source": [
        "Residual blocks introduced the concepts of representational power and representational\n",
        "equivalence. Representational power is a measure of how powerful a block is as a\n",
        "feature extractor. Representational equivalence is the idea that a block can be factored\n",
        "into a lower computational complexity, while maintaining representational power.\n",
        "\n",
        "The design of the residual bottleneck block was demonstrated to maintain representational\n",
        "power of the ResNet34 block, with a lower computational complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqIZIW04vrAA"
      },
      "source": [
        "##Batch normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAfW70Lovrx4"
      },
      "source": [
        "Another problem with adding deeper layers in a neural network is the vanishing gradient\n",
        "problem. This is actually about computer hardware. During training (the process\n",
        "of backward propagation and gradient descent), at each layer the weights are multiplied\n",
        "by very small numbers—specifically, numbers less than 1. As you know, two numbers\n",
        "less than 1 multiplied together make an even smaller number. When these tiny\n",
        "values are propagated through deeper layers, they continuously get smaller. At some\n",
        "point, the computer hardware can’t represent the value anymore—hence, the vanishing\n",
        "gradient.\n",
        "\n",
        "Batch normalization is a technique applied to the output of a layer (before or after\n",
        "the activation function). Without going into the statistics aspect, it normalizes the shift\n",
        "in the weights as they are being trained. This has several advantages: it smooths out\n",
        "(across a batch) the amount of change, thus slowing the possibility of getting a number\n",
        "so small that it can’t be represented by the hardware. Additionally, by narrowing\n",
        "the amount of shift between the weights, convergence can happen sooner by using a\n",
        "higher learning rate and reducing the overall amount of training time. Batch normalization\n",
        "is added to a layer in TF.Keras with the BatchNormalization class.\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same',\n",
        "input_shape=(128, 128, 3)))\n",
        "model.add(BatchNormalization())\n",
        "# Adds batchnorm before the activation\n",
        "model.add(ReLU())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096))\n",
        "model.add(ReLU())\n",
        "\n",
        "# Adds batchnorm after the activation\n",
        "model.add(BatchNormalization())\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPrSVZG3TgvA"
      },
      "source": [
        "##ResNet50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi8wDlg5Tkbx"
      },
      "source": [
        "ResNet50 is a well-known model, which is commonly reused as a stock model, such as\n",
        "for transfer learning, as shared layers in objection detection, and for performance benchmarking.\n",
        "\n",
        "ResNet50 v1 formalized the concept of a convolutional group. This is a set of convolutional\n",
        "blocks that share a common configuration, such as the number of filters. In v1,\n",
        "the neural network is decomposed into groups, and each group doubles the number\n",
        "of filters from the previous group.\n",
        "\n",
        "Additionally, the concept of a separate convolution block to double the number of\n",
        "filters was removed and replaced by a residual block that uses linear projection. Each\n",
        "group starts with a residual block using linear projection on the identity link to double\n",
        "the number of filters, while the remaining residual blocks pass the input directly to the output for the matrix add operation. Additionally, the first 1 × 1 convolution in\n",
        "the residual block with linear projection uses a stride of 2 (feature pooling), which is\n",
        "also known as a strided convolution, reducing the feature map sizes by 75%.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/3-convolutional-and-residual-neural-networks/images/5.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFlIv794UpvV"
      },
      "source": [
        "def identity_block(x, n_filters):\n",
        "  \"\"\"\n",
        "  Create a Bottleneck Residual Block of Convolutions\n",
        "  n_filters: number of filters\n",
        "  x        : input into the block\n",
        "  \"\"\"\n",
        "  shortcut = x\n",
        "\n",
        "  x = Conv2D(n_filters, kernel_size=(1, 1), strides=(1, 1))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  x = Conv2D(n_filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  \n",
        "  x = Conv2D(n_filters * 4, kernel_size=(1, 1), strides=(1, 1))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # Matrix addition of the input to the output\n",
        "  x = layers.add([shortcut, x])\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2cjR--dXK4F"
      },
      "source": [
        "def projection_block(x, n_filters, strides=(2, 2)):\n",
        "  \"\"\"\n",
        "  Create Block of Convolutions with feature pooling\n",
        "  Increase the number of filters by 4X\n",
        "  x        : input into the block\n",
        "  n_filters: number of filters\n",
        "  \"\"\"\n",
        "  # 1 × 1 projection convolution on shortcut to match size of output\n",
        "  shortcut = Conv2D(4 * n_filters, kernel_size=(1, 1), strides=strides)(x)\n",
        "  shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "  x = Conv2D(n_filters, kernel_size=(1, 1), strides=strides)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  x = Conv2D(n_filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  \n",
        "  x = Conv2D(4 * n_filters, kernel_size=(1, 1), strides=(1, 1))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # Matrix addition of the input to the output\n",
        "  x = layers.add([x, shortcut])\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbvtBv9UYRDA"
      },
      "source": [
        "# The input tensor  \n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "\n",
        "# First convolutional layer, where pooled feature maps will be reduced by 75%\n",
        "x = ZeroPadding2D(padding=(3, 3))(inputs)\n",
        "x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding=\"valid\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = ReLU()(x)\n",
        "x = ZeroPadding2D(padding=(1, 1))(x)\n",
        "x = MaxPool2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "\n",
        "# Each convolutional group after the first group starts with a projection block.\n",
        "x = projection_block(64, x, strides=(1, 1))\n",
        "\n",
        "# First identity block group of 64 filters\n",
        "for _ in range(2):\n",
        "  x = identity_block(64, x)\n",
        "x = projection_block(128, x)\n",
        "\n",
        "# Second identity block group of 128 filters\n",
        "for _ in range(3):\n",
        "  x = identity_block(128, x)\n",
        "x = projection_block(256, x)\n",
        "\n",
        "# Third identity block group of 256 filters\n",
        "for _ in range(5):\n",
        "  x = identity_block(256, x)\n",
        "x = projection_block(512, x)\n",
        "\n",
        "# Fourth identity block group of 512 filters\n",
        "for _ in range(2):\n",
        "  x = identity_block(512, x)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Output layer for classification (1000 classes)\n",
        "outputs = Dense(1000, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0U-X4JBizTz"
      },
      "source": [
        "v1.5 introduced a refactoring of the bottleneck design and\n",
        "further reducing of computational complexity, while maintaining representational\n",
        "power. The feature pooling (strides = 2) in the residual block with linear projection is\n",
        "moved from the first 1 × 1 convolution to the 3 × 3 convolution, reducing computational\n",
        "complexity and increasing results on ImageNet by 0.5%.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/3-convolutional-and-residual-neural-networks/images/6.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbF8fQcDjhjV"
      },
      "source": [
        "def projection_block(x, n_filters, strides=(2, 2)):\n",
        "  \"\"\"\n",
        "  Create Block of Convolutions with feature pooling\n",
        "  Increase the number of filters by 4X\n",
        "  x        : input into the block\n",
        "  n_filters: number of filters\n",
        "  \"\"\"\n",
        "  # 1 × 1 projection convolution on shortcut to match size of output\n",
        "  shortcut = Conv2D(4 * n_filters, kernel_size=(1, 1), strides=strides)(x)\n",
        "  shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "  x = Conv2D(n_filters, kernel_size=(1, 1), strides=(1, 1))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  x = Conv2D(n_filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  \n",
        "  x = Conv2D(4 * n_filters, kernel_size=(1, 1), strides=(1, 1))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # Matrix addition of the input to the output\n",
        "  x = layers.add([x, shortcut])\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQZTplvum1Qu"
      },
      "source": [
        "ResNet50 v2 introduced preactivation batch normalization (BN-RE-Conv), in which the batch normalization and activation functions are placed before (instead of after) the corresponding convolution or dense layer. \n",
        "\n",
        "This has now become a common practice, as depicted here for implementation of the residual block with the identity link in v2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8PK93y0m0ey"
      },
      "source": [
        "def identity_block(x, n_filters):\n",
        "  \"\"\"\n",
        "  Create a Bottleneck Residual Block of Convolutions\n",
        "  n_filters: number of filters\n",
        "  x        : input into the block\n",
        "  \"\"\"\n",
        "  shortcut = x\n",
        "\n",
        "  # Batchnorm before the convolution\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  x = Conv2D(n_filters, kernel_size=(1, 1), strides=(1, 1))(x)\n",
        "\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  x = Conv2D(n_filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
        "\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  x = Conv2D(n_filters * 4, kernel_size=(1, 1), strides=(1, 1))(x)\n",
        "\n",
        "  # Matrix addition of the input to the output\n",
        "  x = layers.add([shortcut, x])\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}