{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "invariance-fundamentals.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPqrUY8bfBpwUAV0CmjuFoy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/4-training-fundamentals/invariance_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlFMNBrQMx4o"
      },
      "source": [
        "##Invariance fundamentals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QclXMBouMyyh"
      },
      "source": [
        "So what’s invariance? In the context of neural networks, it means that the outcome\n",
        "(the prediction) is unchanged when the input is transformed. In the context of training\n",
        "an image classifier, image augmentation can be used to train a model to recognize\n",
        "an object regardless of the object’s size and location in the image, without the need\n",
        "for additional training data.\n",
        "\n",
        "Let’s consider a CNN that is an image classifier (this analogy can also be applied to\n",
        "object detection). We want the object being classified to be correctly recognized\n",
        "regardless of its location in the image. If we transform the input so that the object is\n",
        "shifted to a new location in the image, we want the outcome (the prediction) to\n",
        "remain unchanged.\n",
        "\n",
        "For CNNs and imaging in general, the primary types of invariance we want the\n",
        "model to support are translational and scale invariance.\n",
        "\n",
        "One approach to training for translational/scale invariance is simply to have\n",
        "enough images per class (per object), so that the object is in different locations in the image, different rotations, different scales, and different view angles. Well, this may\n",
        "not be practical to collect.\n",
        "\n",
        "It turns out there is a straightforward method of autogenerating translational/\n",
        "scale invariant images using image augmentation preprocessing, which is performed\n",
        "efficiently using matrix operations. \n",
        "\n",
        "Matrix-based transforms can be done by a variety\n",
        "of Python packages, such as the TF.Keras ImageDataGenerator class, TensorFlow\n",
        "tf.image module, or OpenCV.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/4-training-fundamentals/images/9.png?raw=1' width='800'/>\n",
        "\n",
        "It depicts a typical image augmentation pipeline when feeding training\n",
        "data to a model. For each batch drawn, a random subset of the images in the batch are\n",
        "selected for augmentation (for example, 50%). Then, this randomly selected subset of\n",
        "images is randomly transformed according to certain constraints, such as a randomly\n",
        "selected rotation value from –30 to 30 degrees. The modified batch (originals plus\n",
        "augmented) is then fed to the model for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLUeNQMoOZni"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW4XoIp4Oay8"
      },
      "source": [
        "import numpy as np\n",
        "import cv2, imutils\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P5H7SAF_cwW"
      },
      "source": [
        "!wget -q https://github.com/rahiakela/computer-vision-research-and-practice/raw/main/deep-learning-patterns-and-practices/4-training-fundamentals/images/apple.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vokTefxkObFa"
      },
      "source": [
        "##Translational invariance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJFLvgFwQon9"
      },
      "source": [
        "How to manually augment images in a training dataset such\n",
        "that the model learns to recognize the object in the image regardless of its location in\n",
        "the image. \n",
        "\n",
        "For example, we want the model to recognize a horse regardless of which\n",
        "direction the horse faces in the image, or an apple regardless of where in the background\n",
        "the apple is located.\n",
        "\n",
        "Translational invariance in the context of image input includes the following:\n",
        "- Vertical/horizontal location (object can be anywhere in the picture)\n",
        "- Rotation (object can be at any rotation)\n",
        "\n",
        "A vertical/horizontal transformation is typically performed either as a matrix roll\n",
        "operation or a crop. An orientation (for example, mirror) is typically performed as a\n",
        "matrix flip. A rotation is typically handled as a matrix transpose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SxVfmwB85vB"
      },
      "source": [
        "###Image Flip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hRRnp_s887O"
      },
      "source": [
        "A matrix flip transforms an image by flipping it either on the vertical or horizontal axis.\n",
        "Since the image data is represented as a stack of 2D matrices (one per channel), a flip\n",
        "can be done efficiently as a matrix transpose function without changes (such as interpolation)\n",
        "of the pixel data.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/4-training-fundamentals/images/10.png?raw=1' width='800'/>\n",
        "\n",
        "Let’s start by showing how to flip an image by using the popular imaging libraries in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MENfbqvF-XUv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}