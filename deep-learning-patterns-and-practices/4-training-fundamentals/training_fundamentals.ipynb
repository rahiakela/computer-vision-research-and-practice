{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training-fundamentals.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPu249TRe4fPcItjUzPyyS4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/4-training-fundamentals/training_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlFMNBrQMx4o"
      },
      "source": [
        "##Training fundamentals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QclXMBouMyyh"
      },
      "source": [
        "Let’s start with an overview of supervised training. When training a model, you feed\n",
        "data forward through the model, and compute how incorrect the predicted results\n",
        "are—the loss. Then the loss is backward-propagated to make updates to the model’s\n",
        "parameters, which is what the model is learning—the values for the parameters.\n",
        "\n",
        "When training a model, you start with training data that’s representative of the target\n",
        "environment where the model will be deployed. That data, in other words, is a\n",
        "sampling distribution of a population distribution. The training data consists of examples.\n",
        "Each example has two parts: the features, also referred to as independent variables;\n",
        "and corresponding labels, also referred to as the dependent variable.\n",
        "\n",
        "The labels are also known as the ground truths (the “correct answers”). Our goal is\n",
        "to train a model that, once deployed and given examples without labels from the population\n",
        "(examples the model has never seen before), the model is generalized so that\n",
        "it can accurately predict the label (the “correct answer”)—supervised learning. This\n",
        "step is known as inference.\n",
        "\n",
        "During training, we feed batches (also called samples) of the training data to the\n",
        "model through the input layer (also referred to as the bottom of the model). The training\n",
        "data is then transformed by the parameters (weights and biases) in the layers of\n",
        "the model as it moves forward toward the output nodes (also referred to as the top of\n",
        "the model).\n",
        "\n",
        "At the output nodes, we measure how far away we are from the “correct”\n",
        "answers, which, again, is called the loss. We then backward-propagate the loss through\n",
        "the layers of the models and update the parameters to be closer to getting the correct\n",
        "answer on the next batch.\n",
        "\n",
        "We continue to repeat this process until we reach convergence, which could be\n",
        "described as “this is as accurate as we can get on this training run.”\n",
        "\n",
        "**Feeding**\n",
        "\n",
        "Feeding is the process of sampling batches from the training data and forward-feeding\n",
        "the batches through the model, and then calculating the loss at the output. A batch\n",
        "can be one or more examples from the training data chosen at random.\n",
        "\n",
        "The size of the batch is typically constant, which is referred to as the (mini) batch\n",
        "size. All the training data is split into batches, and typically each example will appear in\n",
        "only one batch.\n",
        "\n",
        "All of the training data is fed multiple times to the model. Each time we feed the\n",
        "entire training data, it is called an epoch.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/4-training-fundamentals/images/1.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLUeNQMoOZni"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW4XoIp4Oay8"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, MaxPooling2D, Conv2D, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.datasets import cifar10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vokTefxkObFa"
      },
      "source": [
        "##Backward propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJFLvgFwQon9"
      },
      "source": [
        "After each batch of training data is forward-fed through the model and the loss is calculated,\n",
        "the loss is backward-propagated through the model. We go layer by layer\n",
        "updating the model’s parameters (weights and parameters), starting at the top layer\n",
        "(output) and moving toward the bottom layer (input). How the parameters are\n",
        "updated is a combination of the loss, the values of the current parameters, and the\n",
        "updates made to the proceeding layer.\n",
        "\n",
        "The general method for doing this is based on gradient descent. The optimizer is an\n",
        "implementation of gradient descent whose job is to update the parameters to minimize\n",
        "the loss (maximize getting closer to the correct answer) on subsequent batches.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/4-training-fundamentals/images/2.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUdKDqh4QxYT"
      },
      "source": [
        "##Dataset splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxG61fzIQyRw"
      },
      "source": [
        "A dataset is a collection of examples that are large and diverse enough to be representative\n",
        "of the population being modeled (the sampling distribution). When a dataset\n",
        "meets this definition and is cleaned (not noisy), and in a format that’s ready for machine\n",
        "learning training, we refer to it as a curated dataset.\n",
        "\n",
        "Once you have a curated dataset, the next step is to split it into examples that will\n",
        "be used for training and those that will be used for testing (also called evaluation or\n",
        "holdout). We train the model with the portion of the dataset that is the training data. If\n",
        "we assume the training data is a good sampling distribution (representative of the\n",
        "population distribution), the accuracy of the training data should reflect the accuracy\n",
        "when deployed to the real-world predictions on examples from the population not\n",
        "seen by the model during training.\n",
        "\n",
        "Historically,\n",
        "the rule of thumb has been 80/20: 80% for training and 20% for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfr6TuLtRDBi"
      },
      "source": [
        "###Training and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlof0CuwRDs8"
      },
      "source": [
        "What is important is that we are able to assume our dataset is sufficiently large enough\n",
        "that if we split it into 80% and 20%, and the examples are randomly chosen so that\n",
        "both datasets will be good sampling distributions representative of the population distribution,\n",
        "the model will make predictions (inference) after it’s deployed.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/4-training-fundamentals/images/3.png?raw=1' width='800'/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZGSfjNpOwCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de265250-54ec-4f43-b80f-337ebf4195e8"
      },
      "source": [
        "# Built-in dataset is automatically randomly shuffled and presplit into training and testing data.\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28) (60000,)\n",
            "(10000, 28, 28) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktc0OztcSZ2d"
      },
      "source": [
        "###One-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-q8l1D0Samz"
      },
      "source": [
        "Let’s build a simple DNN to train our curated dataset. We\n",
        "start by flattening the 28-×-28-image input into a 1D vector by using the Flatten layer,\n",
        "which is then followed by two hidden Dense() layers of 512 nodes each, each using\n",
        "the convention of a relu activation function. Finally, the output layer is a Dense layer\n",
        "with 10 nodes, one for each digit. Since this is a multiclass classifier, the activation\n",
        "function for the output layer is a softmax.\n",
        "\n",
        "Next, we compile the model for the convention for multiclass classifiers by using\n",
        "`categorical_crossentropy` for the loss and adam for the optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zlQqVlMOdmg"
      },
      "source": [
        "model = Sequential()\n",
        "# Flattens the 2D grayscale image into 1D vector for a DNN\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "# The actual input layer of the DNN, once the image is flattened\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "# A hidden layer\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "# The output layer of the DNN\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMOwJ4phResN"
      },
      "source": [
        "The most basic way to train this model with this dataset is to use the fit() method. We\n",
        "will pass as parameters the training data (x_train, y_train). We will keep the\n",
        "remaining keyword parameters set to their defaults:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMOCv44JT7bZ"
      },
      "source": [
        "#model.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDa2SP4FftTb"
      },
      "source": [
        "```\n",
        "ValueError: Shapes (32, 1) and (32, 10) are incompatible\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-Ipdm1Lb1Zh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9bead57-be64-4370-9e2a-a8a0dba68175"
      },
      "source": [
        "y_train[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDV_5hH6Y3iz"
      },
      "source": [
        "What went wrong? This is an issue with the loss function we choose. It will compare the\n",
        "difference between each output node and corresponding output expectation. For\n",
        "example, if the answer is the digit 3, we need a 10-element vector (one element per\n",
        "digit) with a 1 (100% probability) in the 3 index and 0s (0% probability) in the remaining\n",
        "indexes. In this case, we need to convert the scalar-value labels into 10-element vectors\n",
        "with a 1 in the corresponding index. This is known as one-hot encoding.\n",
        "\n",
        "Let’s fix our example by first importing the `to_categorical()` function from\n",
        "TF.Keras and then using it to convert the scalar-value labels to one-hot-encoded labels.\n",
        "\n",
        "Note that we pass the value 10 to `to_categorical()` to indicate the size of the onehot-\n",
        "encoded labels (number of classes):\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/4-training-fundamentals/images/4.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eEyB7mcY-0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "440b86b3-e895-4a64-ba4f-b5a61d11c795"
      },
      "source": [
        "# One-hot-encodes the training and testing labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "print(y_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vafsYaMrcTut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91336a12-28fe-42e4-f5d6-e5d2d7f8cd5b"
      },
      "source": [
        "model.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 18s 9ms/step - loss: 1.3116 - accuracy: 0.9108\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f21907bf1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrUBA_vogkSJ"
      },
      "source": [
        "That works, and we got 90% accuracy on the training data—but we can simplify this\n",
        "step. The compile() method has one-hot encoding built into it. To enable it, we just\n",
        "change the loss function from `categorical_crossentropy` to `sparse_categorical_\n",
        "crossentry`. \n",
        "\n",
        "In this mode, the loss function will receive the labels as scalar values and\n",
        "dynamically convert them to one-hot-encoded labels before performing the crossentropy\n",
        "loss calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNiZuowhgsoO"
      },
      "source": [
        "model = Sequential()\n",
        "# Flattens the 2D grayscale image into 1D vector for a DNN\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "# The actual input layer of the DNN, once the image is flattened\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "# A hidden layer\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "# The output layer of the DNN\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8elqoE5g5qk"
      },
      "source": [
        "# Loads MNIST dataset into memory\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4EmQjMrhAOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b66b1a-8a8f-4fa2-d290-f76f1d07e6f6"
      },
      "source": [
        "# Trains MNIST model for 10 epochs\n",
        "model.fit(x_train, y_train, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 1.3454 - accuracy: 0.9072\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.1924 - accuracy: 0.9509\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1703 - accuracy: 0.9551\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1591 - accuracy: 0.9577\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1389 - accuracy: 0.9632\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1265 - accuracy: 0.9682\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1193 - accuracy: 0.9705\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.1067 - accuracy: 0.9749\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.1032 - accuracy: 0.9755\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0844 - accuracy: 0.9798\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2190612f90>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhaH6Kb0R87O"
      },
      "source": [
        "##Data normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM-QS5pC-cDw"
      },
      "source": [
        "Generally, when data feeds forward through the\n",
        "layer and the parameters of one layer are matrix-multiplied against parameters at the\n",
        "next layer, the result is a very small number.\n",
        "\n",
        "The problem with our preceding example is that the input values are substantially\n",
        "larger (up to 255), which will produce large numbers initially as they are multiplied\n",
        "through the layers. This will result in taking longer for the parameters to learn their\n",
        "optimal values—if they learn them at all.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BauFiZil0zD5"
      },
      "source": [
        "###Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEhNlQfF00Cb"
      },
      "source": [
        "We can increase the speed at which the parameters learn the optimal values and\n",
        "increase our chances of convergence (discussed subsequently) by squashing the input\n",
        "values into a smaller range. One simple way to do this is to squash them proportionally\n",
        "into a range from 0 to 1. We can do this by dividing each value by 255.\n",
        "\n",
        "By default, NumPy does floating-point operations as double precision (64 bits). By\n",
        "default, the parameters in a TF.Keras model are single-precision floating-point (32\n",
        "bits). \n",
        "\n",
        "For efficiency, as a last step, we convert the result of the broadcasted division to\n",
        "32 bits by using the NumPy astype() method. If we did not do the conversion, the initial\n",
        "matrix multiplication from the input-to-input layer would take double the number\n",
        "of machine cycles (64 × 32 instead of 32 × 32):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aYs3NyTR_J_"
      },
      "source": [
        "model = Sequential()\n",
        "# Flattens the 2D grayscale image into 1D vector for a DNN\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "# The actual input layer of the DNN, once the image is flattened\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "# A hidden layer\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "# The output layer of the DNN\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9JydF011jYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83cc4ad6-71fc-41a4-8d0b-8b6bce94b583"
      },
      "source": [
        "# Normalizes the pixel data from 0 to 1\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = (x_train / 255.0).astype(np.float32)\n",
        "x_test = (x_test / 255.0).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb5t4ybEtxvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb16c81-f14a-46bf-8c61-74e725620156"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1837 - accuracy: 0.9435\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0812 - accuracy: 0.9748\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0550 - accuracy: 0.9824\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0430 - accuracy: 0.9866\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0345 - accuracy: 0.9893\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0281 - accuracy: 0.9913\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0256 - accuracy: 0.9922\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0218 - accuracy: 0.9930\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0204 - accuracy: 0.9938\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0194 - accuracy: 0.9944\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2209ed2f50>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sEFYuc6vUyi"
      },
      "source": [
        "Let’s compare the output\n",
        "with a normalized input to the prior non-normalized input. In the prior input, we\n",
        "reached 97% accuracy after the 10th epoch. In our normalized input, we reach the\n",
        "same accuracy after just the second epoch, and almost 99.5% accuracy after the tenth.\n",
        "\n",
        "Thus, we learned faster and more accurately when we normalized the input data.\n",
        "\n",
        "The evaluate() method operates in inference mode: the test data is forward-fed\n",
        "through the model to make predictions, but there is no backward propagation. The\n",
        "model’s parameters are not updated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYOXu4T23XTc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec8104b-0fe1-43e1-9ce0-fec14e17614b"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1021 - accuracy: 0.9799\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.10212759673595428, 0.9799000024795532]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou9Akf1m3lyk"
      },
      "source": [
        "We see that the accuracy is about 98%, compared to the training\n",
        "accuracy of 99.5%. This is expected. Some overfitting always occurs during training.\n",
        "What we are looking for is a very small difference between the training and testing, and\n",
        "in this case it’s about 1.5%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS9xyM-X3qaw"
      },
      "source": [
        "###Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeCB3bzc3rJv"
      },
      "source": [
        "Another technique, called standardization, is\n",
        "considered to give a better result. However, it requires a pre-analysis (scan) over the\n",
        "entire input data to find its mean and standard deviation. You then center the data at\n",
        "the mean of the full distribution of the input data and squash the values between +/–\n",
        "one standard deviation.\n",
        "\n",
        "The following code, which implements standardization when\n",
        "the input data is in memory as a NumPy multidimensional array, uses the NumPy\n",
        "methods np.mean() and np.std():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRAUskXb4ZiZ"
      },
      "source": [
        "# Calculates the mean value for the pixel data\n",
        "mean = np.mean(x_train)\n",
        "# Calculates the standard deviation for the pixel data\n",
        "std = np.std(x_train)\n",
        "\n",
        "# Standardization of the pixel data using the mean and standard deviation\n",
        "x_train = ((x_train - mean) / std).astype(np.float32)\n",
        "x_test = ((x_test - mean) / std).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePxqiydS5DsI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f2ec79-9067-45bd-b96b-a3e9ef496170"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.1058 - accuracy: 0.9847\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0336 - accuracy: 0.9924\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0354 - accuracy: 0.9922\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0385 - accuracy: 0.9915\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0333 - accuracy: 0.9928\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0350 - accuracy: 0.9922\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0326 - accuracy: 0.9928\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0316 - accuracy: 0.9933\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0264 - accuracy: 0.9941\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0307 - accuracy: 0.9937\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2209de2410>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUbLfcot5Grg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5dc8c3-a2bd-4351-96b3-e86b6ca63fe4"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1868 - accuracy: 0.9796\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1867692768573761, 0.9796000123023987]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqIZIW04vrAA"
      },
      "source": [
        "##Validation and overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAfW70Lovrx4"
      },
      "source": [
        "Typically, to get higher accuracy, we build larger and larger models. One consequence\n",
        "is that the model can rote-memorize some or all of the examples. The\n",
        "model learns the examples instead of learning to generalize from the examples to\n",
        "accurately predict examples it never saw during training. \n",
        "\n",
        "In an extreme case, a model\n",
        "could achieve 100% training accuracy yet have random accuracy on the testing (for 10\n",
        "classes, that would be 10% accuracy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18BCfjmFzmaZ"
      },
      "source": [
        "###Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7nwzwAcznP6"
      },
      "source": [
        "Let’s say training the model takes several hours. Do you really want to wait until the\n",
        "end of training and then test on the test data to learn whether the model overfitted?\n",
        "Of course not. Instead, we set aside a small portion of the training data, which we call\n",
        "validation data.\n",
        "\n",
        "We don’t train the model with the validation data. Instead, after each epoch, we\n",
        "use the validation data to estimate the likely result on the test data. Like the test data, the validation data is forward-fed through the model without updating the model’s\n",
        "parameters (inference mode), and we measure the loss and accuracy. \n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/4-training-fundamentals/images/5.png?raw=1' width='800'/>\n",
        "\n",
        "If a dataset is very small, and using even less data for training has a negative impact, we\n",
        "can use cross-validation\n",
        "\n",
        "At\n",
        "the beginning of each epoch, the examples for validation are randomly selected and\n",
        "not used for training for that epoch, and instead used for the validation test. But since\n",
        "the selection is random, some or all of the examples will appear in the training data\n",
        "for other epochs. Today’s datasets are large, so you seldom see the need for this tech\n",
        "nique.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/4-training-fundamentals/images/6.png?raw=1' width='800'/>\n",
        "\n",
        "Next, we will train a simple CNN to classify images from the CIFAR-10 dataset. Our\n",
        "dataset is a subset of this dataset of tiny images, of size 32 × 32 × 3. It consists of 60,000\n",
        "training and 10,000 test images covering 10 classes: airplane, automobile, bird, cat,\n",
        "deer, dog, frog, horse, ship, and truck.\n",
        "\n",
        "In our simple CNN, we have one convolutional layer of 32 filters with kernel size\n",
        "3 × 3, followed by a strided max pooling layer. The output is then flattened and passed\n",
        "to the final outputting dense layer.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/4-training-fundamentals/images/7.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sEi98S51qjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe81b44-a14b-4fcc-de1f-6248cb54f8a4"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 7200)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                72010     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 72,906\n",
            "Trainable params: 72,906\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bvp9GkP4-iE"
      },
      "source": [
        "Here, we’ve added the keyword parameter validation_split=0.1 to the fit()\n",
        "method to set aside 10% of the training data for validation testing after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d3V7n044UwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1e3019a-574e-40b4-c28b-8680672b329e"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = (x_train / 255.0).astype(np.float32)\n",
        "x_test = (x_test / 255.0).astype(np.float32)\n",
        "\n",
        "# Uses 10% of training data for validation—not trained on\n",
        "model.fit(x_train, y_train, epochs=15, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1407/1407 [==============================] - 27s 19ms/step - loss: 1.2080 - accuracy: 0.5814 - val_loss: 1.1735 - val_accuracy: 0.6066\n",
            "Epoch 2/15\n",
            "1407/1407 [==============================] - 27s 19ms/step - loss: 1.1121 - accuracy: 0.6140 - val_loss: 1.1403 - val_accuracy: 0.6042\n",
            "Epoch 3/15\n",
            "1407/1407 [==============================] - 27s 20ms/step - loss: 1.0493 - accuracy: 0.6399 - val_loss: 1.1211 - val_accuracy: 0.6122\n",
            "Epoch 4/15\n",
            "1407/1407 [==============================] - 27s 19ms/step - loss: 0.9906 - accuracy: 0.6596 - val_loss: 1.0711 - val_accuracy: 0.6308\n",
            "Epoch 5/15\n",
            "1407/1407 [==============================] - 28s 20ms/step - loss: 0.9480 - accuracy: 0.6732 - val_loss: 1.0979 - val_accuracy: 0.6250\n",
            "Epoch 6/15\n",
            "1407/1407 [==============================] - 27s 19ms/step - loss: 0.9109 - accuracy: 0.6864 - val_loss: 1.0758 - val_accuracy: 0.6356\n",
            "Epoch 7/15\n",
            "1407/1407 [==============================] - 28s 20ms/step - loss: 0.8766 - accuracy: 0.6989 - val_loss: 1.0426 - val_accuracy: 0.6454\n",
            "Epoch 8/15\n",
            "1407/1407 [==============================] - 28s 20ms/step - loss: 0.8513 - accuracy: 0.7080 - val_loss: 1.0596 - val_accuracy: 0.6422\n",
            "Epoch 9/15\n",
            "1407/1407 [==============================] - 28s 20ms/step - loss: 0.8328 - accuracy: 0.7120 - val_loss: 1.0558 - val_accuracy: 0.6486\n",
            "Epoch 10/15\n",
            "1407/1407 [==============================] - 27s 19ms/step - loss: 0.8084 - accuracy: 0.7232 - val_loss: 1.0243 - val_accuracy: 0.6600\n",
            "Epoch 11/15\n",
            "1407/1407 [==============================] - 27s 19ms/step - loss: 0.7860 - accuracy: 0.7286 - val_loss: 1.0470 - val_accuracy: 0.6550\n",
            "Epoch 12/15\n",
            "1407/1407 [==============================] - 27s 19ms/step - loss: 0.7701 - accuracy: 0.7334 - val_loss: 1.0397 - val_accuracy: 0.6610\n",
            "Epoch 13/15\n",
            "1407/1407 [==============================] - 27s 19ms/step - loss: 0.7507 - accuracy: 0.7389 - val_loss: 1.0710 - val_accuracy: 0.6488\n",
            "Epoch 14/15\n",
            "1407/1407 [==============================] - 27s 19ms/step - loss: 0.7357 - accuracy: 0.7446 - val_loss: 1.0736 - val_accuracy: 0.6468\n",
            "Epoch 15/15\n",
            "1407/1407 [==============================] - 27s 19ms/step - loss: 0.7128 - accuracy: 0.7534 - val_loss: 1.0770 - val_accuracy: 0.6562\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc51e282dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvM73jUX5eiU"
      },
      "source": [
        "You can see that after the\n",
        "fourth epoch, the training and evaluation accuracy are essentially the same. But after\n",
        "the fifth epoch, we start to see them spread apart (65% versus 61%). By the 15th epoch, the spread is very large (74% versus 63%). Our model clearly started overfitting\n",
        "around the fifth epoch.\n",
        "\n",
        "Let’s now work on getting the model to not overfit to the examples and instead generalize\n",
        "from them.So, we want to add some regularization—\n",
        "some noise—during training so the model cannot rote-memorize the training\n",
        "examples.\n",
        "\n",
        "In this code example, we modify our model by adding 50% dropout before\n",
        "the final dense layer. Because dropout will slow our learning (because of forgetting),\n",
        "we increase the number of epochs to 20:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ky--ghN5jlu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cb241fe-1270-404b-9da1-285d05006018"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "# Adds noise to training to prevent overfitting\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Uses 10% of training data for validation—not trained on\n",
        "model.fit(x_train, y_train, epochs=20, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1407/1407 [==============================] - 32s 23ms/step - loss: 1.5527 - accuracy: 0.4530 - val_loss: 1.3208 - val_accuracy: 0.5528\n",
            "Epoch 2/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.3043 - accuracy: 0.5432 - val_loss: 1.2488 - val_accuracy: 0.5724\n",
            "Epoch 3/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2301 - accuracy: 0.5731 - val_loss: 1.2169 - val_accuracy: 0.5786\n",
            "Epoch 4/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1838 - accuracy: 0.5910 - val_loss: 1.1245 - val_accuracy: 0.6188\n",
            "Epoch 5/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1522 - accuracy: 0.6022 - val_loss: 1.1112 - val_accuracy: 0.6276\n",
            "Epoch 6/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1238 - accuracy: 0.6104 - val_loss: 1.0903 - val_accuracy: 0.6304\n",
            "Epoch 7/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1099 - accuracy: 0.6184 - val_loss: 1.0745 - val_accuracy: 0.6374\n",
            "Epoch 8/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0851 - accuracy: 0.6239 - val_loss: 1.0944 - val_accuracy: 0.6288\n",
            "Epoch 9/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0737 - accuracy: 0.6279 - val_loss: 1.0530 - val_accuracy: 0.6436\n",
            "Epoch 10/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0588 - accuracy: 0.6356 - val_loss: 1.0619 - val_accuracy: 0.6390\n",
            "Epoch 11/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0468 - accuracy: 0.6370 - val_loss: 1.0415 - val_accuracy: 0.6458\n",
            "Epoch 12/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0434 - accuracy: 0.6414 - val_loss: 1.0323 - val_accuracy: 0.6542\n",
            "Epoch 13/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0371 - accuracy: 0.6435 - val_loss: 1.0564 - val_accuracy: 0.6456\n",
            "Epoch 14/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0255 - accuracy: 0.6433 - val_loss: 1.0293 - val_accuracy: 0.6500\n",
            "Epoch 15/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0178 - accuracy: 0.6480 - val_loss: 1.0331 - val_accuracy: 0.6514\n",
            "Epoch 16/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0091 - accuracy: 0.6514 - val_loss: 1.0049 - val_accuracy: 0.6590\n",
            "Epoch 17/20\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0111 - accuracy: 0.6506 - val_loss: 1.0397 - val_accuracy: 0.6492\n",
            "Epoch 18/20\n",
            "1407/1407 [==============================] - 32s 23ms/step - loss: 1.0010 - accuracy: 0.6561 - val_loss: 1.0099 - val_accuracy: 0.6606\n",
            "Epoch 19/20\n",
            "1407/1407 [==============================] - 33s 23ms/step - loss: 0.9995 - accuracy: 0.6555 - val_loss: 1.0083 - val_accuracy: 0.6604\n",
            "Epoch 20/20\n",
            "1407/1407 [==============================] - 33s 23ms/step - loss: 0.9979 - accuracy: 0.6540 - val_loss: 1.0241 - val_accuracy: 0.6596\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc5228ea290>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxKYutbh7Coc"
      },
      "source": [
        "Thus, the\n",
        "model is learning to generalize instead of rote-memorizing the training examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPrSVZG3TgvA"
      },
      "source": [
        "###Going deeper with layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi8wDlg5Tkbx"
      },
      "source": [
        "As we know, simply going deeper with layers can lead to instability\n",
        "in the model, without addressing the issues with techniques such as identity links\n",
        "and batch normalization.\n",
        "\n",
        "For example, many of the values we are matrix-multiplying\n",
        "are small numbers less than 1. Multiply two numbers less than 1, and you get an even\n",
        "smaller number. At some point, numbers get so small that the hardware can’t represent\n",
        "the value anymore, which is referred to as a vanishing gradient. \n",
        "\n",
        "In other cases, the\n",
        "parameters may be too close to distinguish from each other—or the opposite, spread\n",
        "too far apart, which is referred to as an exploding gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFlIv794UpvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5db2cc2-fd89-4a7e-acbd-1c0451e0105c"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "for _ in range(40):\n",
        "  model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = (x_train / 255.0).astype(np.float32)\n",
        "x_test = (x_test / 255.0).astype(np.float32)\n",
        "\n",
        "# Uses 10% of training data for validation—not trained on\n",
        "model.fit(x_train, y_train, epochs=10, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 17s 9ms/step - loss: 2.3014 - accuracy: 0.1120 - val_loss: 2.3021 - val_accuracy: 0.1050\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 14s 9ms/step - loss: 2.3013 - accuracy: 0.1132 - val_loss: 2.3019 - val_accuracy: 0.1050\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 15s 9ms/step - loss: 2.3013 - accuracy: 0.1132 - val_loss: 2.3019 - val_accuracy: 0.1050\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 15s 9ms/step - loss: 2.3013 - accuracy: 0.1132 - val_loss: 2.3022 - val_accuracy: 0.1050\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 15s 9ms/step - loss: 2.3013 - accuracy: 0.1132 - val_loss: 2.3020 - val_accuracy: 0.1050\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 15s 9ms/step - loss: 2.3013 - accuracy: 0.1132 - val_loss: 2.3021 - val_accuracy: 0.1050\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 15s 9ms/step - loss: 2.3012 - accuracy: 0.1132 - val_loss: 2.3019 - val_accuracy: 0.1050\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 14s 8ms/step - loss: 2.3013 - accuracy: 0.1132 - val_loss: 2.3019 - val_accuracy: 0.1050\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 15s 9ms/step - loss: 2.3013 - accuracy: 0.1132 - val_loss: 2.3020 - val_accuracy: 0.1050\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 14s 8ms/step - loss: 2.3013 - accuracy: 0.1132 - val_loss: 2.3022 - val_accuracy: 0.1050\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9ad38fb850>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9B8ZGi8F78R"
      },
      "source": [
        "You can see in the first three epochs we have a consistent\n",
        "increase in accuracy in training and evaluation data, as well as a consistent decrease in\n",
        "corresponding loss. But afterward, the accuracy becomes erratic; the model is numerically\n",
        "unstable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0U-X4JBizTz"
      },
      "source": [
        "##Convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrPrPvNGcLUc"
      },
      "source": [
        "We now look for convergence on an acceptable local optimum based on how the model will be used in an application. If we overtrain the neural network, the following can happen:\n",
        "\n",
        "- The neural network becomes overfitted to the training data, showing increasing\n",
        "accuracy on the training data, but degrading accuracy on the testing data.\n",
        "- In deeper neural networks, the layers will learn in a nonuniform manner and\n",
        "have different convergence rates. Thus, as some layers are working toward convergence,\n",
        "others may have convergence and thus start diverging.\n",
        "- Continued training may cause the neural network to pop out of one local optimum\n",
        "and start converging on another that is less accurate.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/4-training-fundamentals/images/8.png?raw=1' width='800'/>\n",
        "\n",
        "You start with a fairly fast reduction in loss across the early epochs, and as training\n",
        "homes in on a (near) optimal optimum, the rate of reduction slows, and then finally\n",
        "plateaus—at which point, you have convergence.\n",
        "\n",
        "Let’s start with a simple ConvNet model in TF.Keras using the CIFAR-10 dataset to\n",
        "demonstrate the concept of convergence and then diverging. \n",
        "\n",
        "In this code, I have intentionally\n",
        "left out methods that prevent overfitting, like dropout or batch normalization:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbF8fQcDjhjV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c57cd92-0d69-452e-ed3d-9f93f1526090"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Calculates the height and width of the images in the dataset\n",
        "height = x_train.shape[1]\n",
        "width = x_train.shape[2]\n",
        "\n",
        "# Normalizes the input data\n",
        "x_train = (x_train / 255.).astype(np.float32)\n",
        "x_test = (x_test / 255.).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "170508288/170498071 [==============================] - 6s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmv78Y7QdQZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae911e27-34ff-48c0-e82f-602c08678145"
      },
      "source": [
        "model = Sequential()\n",
        "# Sets the input shape to the model to the height and width of the images in the dataset\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(height, width, 3)))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=20, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1407/1407 [==============================] - 42s 29ms/step - loss: 1.4802 - accuracy: 0.4741 - val_loss: 1.2544 - val_accuracy: 0.5662\n",
            "Epoch 2/20\n",
            "1407/1407 [==============================] - 40s 28ms/step - loss: 1.1805 - accuracy: 0.5870 - val_loss: 1.1774 - val_accuracy: 0.5886\n",
            "Epoch 3/20\n",
            "1407/1407 [==============================] - 40s 28ms/step - loss: 1.0585 - accuracy: 0.6298 - val_loss: 1.0547 - val_accuracy: 0.6398\n",
            "Epoch 4/20\n",
            "1407/1407 [==============================] - 40s 28ms/step - loss: 0.9652 - accuracy: 0.6614 - val_loss: 1.0399 - val_accuracy: 0.6438\n",
            "Epoch 5/20\n",
            "1407/1407 [==============================] - 39s 28ms/step - loss: 0.8988 - accuracy: 0.6836 - val_loss: 1.0503 - val_accuracy: 0.6438\n",
            "Epoch 6/20\n",
            "1407/1407 [==============================] - 40s 28ms/step - loss: 0.8425 - accuracy: 0.7040 - val_loss: 1.0008 - val_accuracy: 0.6656\n",
            "Epoch 7/20\n",
            "1407/1407 [==============================] - 40s 28ms/step - loss: 0.7863 - accuracy: 0.7241 - val_loss: 1.0512 - val_accuracy: 0.6524\n",
            "Epoch 8/20\n",
            "1407/1407 [==============================] - 39s 28ms/step - loss: 0.7406 - accuracy: 0.7412 - val_loss: 1.0286 - val_accuracy: 0.6598\n",
            "Epoch 9/20\n",
            "1407/1407 [==============================] - 39s 28ms/step - loss: 0.6848 - accuracy: 0.7599 - val_loss: 1.0593 - val_accuracy: 0.6592\n",
            "Epoch 10/20\n",
            "1407/1407 [==============================] - 38s 27ms/step - loss: 0.6436 - accuracy: 0.7736 - val_loss: 1.1165 - val_accuracy: 0.6468\n",
            "Epoch 11/20\n",
            "1407/1407 [==============================] - 38s 27ms/step - loss: 0.6048 - accuracy: 0.7888 - val_loss: 1.1218 - val_accuracy: 0.6540\n",
            "Epoch 12/20\n",
            "1407/1407 [==============================] - 38s 27ms/step - loss: 0.5613 - accuracy: 0.8041 - val_loss: 1.1519 - val_accuracy: 0.6512\n",
            "Epoch 13/20\n",
            "1407/1407 [==============================] - 38s 27ms/step - loss: 0.5262 - accuracy: 0.8154 - val_loss: 1.1916 - val_accuracy: 0.6498\n",
            "Epoch 14/20\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.4895 - accuracy: 0.8264 - val_loss: 1.2157 - val_accuracy: 0.6480\n",
            "Epoch 15/20\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.4561 - accuracy: 0.8399 - val_loss: 1.3216 - val_accuracy: 0.6378\n",
            "Epoch 16/20\n",
            "1407/1407 [==============================] - 38s 27ms/step - loss: 0.4234 - accuracy: 0.8499 - val_loss: 1.3803 - val_accuracy: 0.6352\n",
            "Epoch 17/20\n",
            "1407/1407 [==============================] - 38s 27ms/step - loss: 0.3902 - accuracy: 0.8637 - val_loss: 1.4371 - val_accuracy: 0.6314\n",
            "Epoch 18/20\n",
            "1407/1407 [==============================] - 38s 27ms/step - loss: 0.3593 - accuracy: 0.8732 - val_loss: 1.4769 - val_accuracy: 0.6312\n",
            "Epoch 19/20\n",
            "1407/1407 [==============================] - 38s 27ms/step - loss: 0.3290 - accuracy: 0.8851 - val_loss: 1.5607 - val_accuracy: 0.6320\n",
            "Epoch 20/20\n",
            "1407/1407 [==============================] - 38s 27ms/step - loss: 0.3017 - accuracy: 0.8948 - val_loss: 1.6364 - val_accuracy: 0.6266\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd50ebd3ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQZTplvum1Qu"
      },
      "source": [
        "You can see a steady reduction in loss with\n",
        "each pass, which means the neural network is getting closer to fitting the data. Additionally,\n",
        "the accuracy on the training data is going up from 52.35% to 87.4%, and on\n",
        "the validation data it’s increasing from 63.46% to 67.14%.\n",
        "\n",
        "Let’s now look at epochs 11 through 20. You can see that we’ve hit 98.46% on the\n",
        "training data, which means we are tightly fitted to it. On the other hand, our accuracy\n",
        "on the validation data plateaued at 66.58%. Thus, after six epochs, continued training\n",
        "provided no improvement, and we can conclude that by epoch 7, the model was overfitted\n",
        "to the training data.\n",
        "\n",
        "The values of the loss function for the training and validation data also indicate that\n",
        "the model is overfitting. The loss function between epochs 11 and 20 for the training\n",
        "data continues to get smaller, but for the corresponding validation data, it plateaus\n",
        "and then gets worse (diverges)."
      ]
    }
  ]
}