{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training-fundamentals.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN3YUOYZ9wb+lR/MIz4v6KC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/4-training-fundamentals/training_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlFMNBrQMx4o"
      },
      "source": [
        "##Training fundamentals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QclXMBouMyyh"
      },
      "source": [
        "Let’s start with an overview of supervised training. When training a model, you feed\n",
        "data forward through the model, and compute how incorrect the predicted results\n",
        "are—the loss. Then the loss is backward-propagated to make updates to the model’s\n",
        "parameters, which is what the model is learning—the values for the parameters.\n",
        "\n",
        "When training a model, you start with training data that’s representative of the target\n",
        "environment where the model will be deployed. That data, in other words, is a\n",
        "sampling distribution of a population distribution. The training data consists of examples.\n",
        "Each example has two parts: the features, also referred to as independent variables;\n",
        "and corresponding labels, also referred to as the dependent variable.\n",
        "\n",
        "The labels are also known as the ground truths (the “correct answers”). Our goal is\n",
        "to train a model that, once deployed and given examples without labels from the population\n",
        "(examples the model has never seen before), the model is generalized so that\n",
        "it can accurately predict the label (the “correct answer”)—supervised learning. This\n",
        "step is known as inference.\n",
        "\n",
        "During training, we feed batches (also called samples) of the training data to the\n",
        "model through the input layer (also referred to as the bottom of the model). The training\n",
        "data is then transformed by the parameters (weights and biases) in the layers of\n",
        "the model as it moves forward toward the output nodes (also referred to as the top of\n",
        "the model).\n",
        "\n",
        "At the output nodes, we measure how far away we are from the “correct”\n",
        "answers, which, again, is called the loss. We then backward-propagate the loss through\n",
        "the layers of the models and update the parameters to be closer to getting the correct\n",
        "answer on the next batch.\n",
        "\n",
        "We continue to repeat this process until we reach convergence, which could be\n",
        "described as “this is as accurate as we can get on this training run.”\n",
        "\n",
        "**Feeding**\n",
        "\n",
        "Feeding is the process of sampling batches from the training data and forward-feeding\n",
        "the batches through the model, and then calculating the loss at the output. A batch\n",
        "can be one or more examples from the training data chosen at random.\n",
        "\n",
        "The size of the batch is typically constant, which is referred to as the (mini) batch\n",
        "size. All the training data is split into batches, and typically each example will appear in\n",
        "only one batch.\n",
        "\n",
        "All of the training data is fed multiple times to the model. Each time we feed the\n",
        "entire training data, it is called an epoch.\n",
        "\n",
        "<img src='images/1.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLUeNQMoOZni"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW4XoIp4Oay8"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vokTefxkObFa"
      },
      "source": [
        "##Backward propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJFLvgFwQon9"
      },
      "source": [
        "After each batch of training data is forward-fed through the model and the loss is calculated,\n",
        "the loss is backward-propagated through the model. We go layer by layer\n",
        "updating the model’s parameters (weights and parameters), starting at the top layer\n",
        "(output) and moving toward the bottom layer (input). How the parameters are\n",
        "updated is a combination of the loss, the values of the current parameters, and the\n",
        "updates made to the proceeding layer.\n",
        "\n",
        "The general method for doing this is based on gradient descent. The optimizer is an\n",
        "implementation of gradient descent whose job is to update the parameters to minimize\n",
        "the loss (maximize getting closer to the correct answer) on subsequent batches.\n",
        "\n",
        "<img src='images/2.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUdKDqh4QxYT"
      },
      "source": [
        "##Dataset splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxG61fzIQyRw"
      },
      "source": [
        "A dataset is a collection of examples that are large and diverse enough to be representative\n",
        "of the population being modeled (the sampling distribution). When a dataset\n",
        "meets this definition and is cleaned (not noisy), and in a format that’s ready for machine\n",
        "learning training, we refer to it as a curated dataset.\n",
        "\n",
        "Once you have a curated dataset, the next step is to split it into examples that will\n",
        "be used for training and those that will be used for testing (also called evaluation or\n",
        "holdout). We train the model with the portion of the dataset that is the training data. If\n",
        "we assume the training data is a good sampling distribution (representative of the\n",
        "population distribution), the accuracy of the training data should reflect the accuracy\n",
        "when deployed to the real-world predictions on examples from the population not\n",
        "seen by the model during training.\n",
        "\n",
        "Historically,\n",
        "the rule of thumb has been 80/20: 80% for training and 20% for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfr6TuLtRDBi"
      },
      "source": [
        "###Training and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlof0CuwRDs8"
      },
      "source": [
        "What is important is that we are able to assume our dataset is sufficiently large enough\n",
        "that if we split it into 80% and 20%, and the examples are randomly chosen so that\n",
        "both datasets will be good sampling distributions representative of the population distribution,\n",
        "the model will make predictions (inference) after it’s deployed.\n",
        "\n",
        "<img src='images/3.png?raw=1' width='800'/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZGSfjNpOwCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b71407-f4a0-40e2-f0fa-6138600bfb99"
      },
      "source": [
        "# Built-in dataset is automatically randomly shuffled and presplit into training and testing data.\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000,)\n",
            "(10000, 28, 28) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktc0OztcSZ2d"
      },
      "source": [
        "###One-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-q8l1D0Samz"
      },
      "source": [
        "Let’s build a simple DNN to train our curated dataset. We\n",
        "start by flattening the 28-×-28-image input into a 1D vector by using the Flatten layer,\n",
        "which is then followed by two hidden Dense() layers of 512 nodes each, each using\n",
        "the convention of a relu activation function. Finally, the output layer is a Dense layer\n",
        "with 10 nodes, one for each digit. Since this is a multiclass classifier, the activation\n",
        "function for the output layer is a softmax.\n",
        "\n",
        "Next, we compile the model for the convention for multiclass classifiers by using\n",
        "`categorical_crossentropy` for the loss and adam for the optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zlQqVlMOdmg"
      },
      "source": [
        "model = Sequential()\n",
        "# Flattens the 2D grayscale image into 1D vector for a DNN\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "# The actual input layer of the DNN, once the image is flattened\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "# A hidden layer\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "# The output layer of the DNN\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMOwJ4phResN"
      },
      "source": [
        "The most basic way to train this model with this dataset is to use the fit() method. We\n",
        "will pass as parameters the training data (x_train, y_train). We will keep the\n",
        "remaining keyword parameters set to their defaults:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "HMOCv44JT7bZ",
        "outputId": "8fa323b4-089c-4f36-9591-b3d1b4098732"
      },
      "source": [
        "model.fit(x_train, y_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-139fa7af76d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 810, in train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1665, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 4994, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 1) and (32, 10) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhaH6Kb0R87O"
      },
      "source": [
        "##ResNet with bottleneck residual block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM-QS5pC-cDw"
      },
      "source": [
        "ResNet50 introduced a variation of the residual block referred to as the bottleneck\n",
        "residual block. In this version, the group of two 3 × 3 convolutional layers is replaced by\n",
        "a group of 1 × 1, then 3 × 3, and then 1 × 1 convolutional layers. \n",
        "\n",
        "The first 1 × 1 convolution\n",
        "performs a dimensionality reduction, reducing the computational complexity,\n",
        "and the last convolution restores the dimensionality, increasing the number of filters\n",
        "by a factor of 4. \n",
        "\n",
        "The middle 3 × 3 convolution is referred to as the bottleneck convolution,\n",
        "like the neck of a bottle. \n",
        "\n",
        "The bottleneck residual block, allows\n",
        "for deeper neural networks, without degradation, and further reduction in computational\n",
        "complexity.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/3-convolutional-and-residual-neural-networks/images/4.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aYs3NyTR_J_"
      },
      "source": [
        "def bottleneck_block(n_filters, x):\n",
        "  \"\"\"\n",
        "  Create a Bottleneck Residual Block of Convolutions\n",
        "  n_filters: number of filters\n",
        "  x        : input into the block\n",
        "  \"\"\"\n",
        "  shortcut = x\n",
        "  # A 1 × 1 bottleneck convolution for dimensionality reduction\n",
        "  x = Conv2D(n_filters, kernel_size=(1, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(x)\n",
        "  # A 3 × 3 convolution for feature extraction\n",
        "  x = Conv2D(n_filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", activation=\"relu\")(x)\n",
        "  # A 1 × 1 projection convolution for dimensionality expansion\n",
        "  x = Conv2D(n_filters * 4, kernel_size=(1, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(x)\n",
        "  # Matrix addition of the input to the output\n",
        "  x = layers.add([shortcut, x])\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb5t4ybEtxvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94467e6e-c72e-4938-ee6d-dc79b555f52b"
      },
      "source": [
        "# The input tensor  \n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "\n",
        "# First convolutional layer, where pooled feature maps will be reduced by 75%\n",
        "x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding=\"same\", activation=\"relu\")(inputs)\n",
        "x = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(x)\n",
        "\n",
        "# First residual block group of 64 filters\n",
        "for _ in range(2):\n",
        "  x = residual_block(64, x)\n",
        "\n",
        "# Doubles the size of filters and reduces feature maps by 75% (stride s = 2, 2) to fit the next residual group\n",
        "x = conv_block(128, x)\n",
        "\n",
        "# Second residual block group of 128 filters\n",
        "for _ in range(3):\n",
        "  x = residual_block(128, x)\n",
        "\n",
        "x = conv_block(256, x)\n",
        "\n",
        "# Third residual block group of 256 filters\n",
        "for _ in range(5):\n",
        "  x = residual_block(256, x)\n",
        "\n",
        "x = conv_block(512, x)\n",
        "\n",
        "# Fourth residual block group of 512 filters\n",
        "x = residual_block(512, x)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Output layer for classification (1000 classes)\n",
        "outputs = Dense(1000, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 112, 112, 64) 9472        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 56, 56, 64)   0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 56, 56, 64)   36928       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 56, 56, 64)   36928       conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 56, 56, 64)   0           max_pooling2d_2[0][0]            \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 56, 56, 64)   36928       add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 56, 56, 64)   36928       conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 56, 56, 64)   0           add_12[0][0]                     \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 28, 28, 128)  73856       add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 14, 14, 128)  147584      conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 14, 14, 128)  147584      conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 14, 14, 128)  147584      conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 14, 14, 128)  0           conv2d_39[0][0]                  \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 14, 14, 128)  147584      add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 14, 14, 128)  147584      conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 14, 14, 128)  0           add_14[0][0]                     \n",
            "                                                                 conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 14, 14, 128)  147584      add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 14, 14, 128)  147584      conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 14, 14, 128)  0           add_15[0][0]                     \n",
            "                                                                 conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 7, 7, 256)    295168      add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 4, 4, 256)    0           conv2d_47[0][0]                  \n",
            "                                                                 conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 4, 4, 256)    590080      add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 4, 4, 256)    0           add_17[0][0]                     \n",
            "                                                                 conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 4, 4, 256)    590080      add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 4, 4, 256)    0           add_18[0][0]                     \n",
            "                                                                 conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 4, 4, 256)    590080      add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 4, 4, 256)    0           add_19[0][0]                     \n",
            "                                                                 conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 4, 4, 256)    590080      add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 4, 4, 256)    590080      conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 4, 4, 256)    0           add_20[0][0]                     \n",
            "                                                                 conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 2, 2, 512)    1180160     add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 1, 1, 512)    2359808     conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 1, 1, 512)    2359808     conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 1, 1, 512)    2359808     conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 1, 1, 512)    0           conv2d_59[0][0]                  \n",
            "                                                                 conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 512)          0           add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1000)         513000      global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 16,822,760\n",
            "Trainable params: 16,822,760\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sEFYuc6vUyi"
      },
      "source": [
        "Residual blocks introduced the concepts of representational power and representational\n",
        "equivalence. Representational power is a measure of how powerful a block is as a\n",
        "feature extractor. Representational equivalence is the idea that a block can be factored\n",
        "into a lower computational complexity, while maintaining representational power.\n",
        "\n",
        "The design of the residual bottleneck block was demonstrated to maintain representational\n",
        "power of the ResNet34 block, with a lower computational complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqIZIW04vrAA"
      },
      "source": [
        "##Batch normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAfW70Lovrx4"
      },
      "source": [
        "Another problem with adding deeper layers in a neural network is the vanishing gradient\n",
        "problem. This is actually about computer hardware. During training (the process\n",
        "of backward propagation and gradient descent), at each layer the weights are multiplied\n",
        "by very small numbers—specifically, numbers less than 1. As you know, two numbers\n",
        "less than 1 multiplied together make an even smaller number. When these tiny\n",
        "values are propagated through deeper layers, they continuously get smaller. At some\n",
        "point, the computer hardware can’t represent the value anymore—hence, the vanishing\n",
        "gradient.\n",
        "\n",
        "Batch normalization is a technique applied to the output of a layer (before or after\n",
        "the activation function). Without going into the statistics aspect, it normalizes the shift\n",
        "in the weights as they are being trained. This has several advantages: it smooths out\n",
        "(across a batch) the amount of change, thus slowing the possibility of getting a number\n",
        "so small that it can’t be represented by the hardware. Additionally, by narrowing\n",
        "the amount of shift between the weights, convergence can happen sooner by using a\n",
        "higher learning rate and reducing the overall amount of training time. Batch normalization\n",
        "is added to a layer in TF.Keras with the BatchNormalization class.\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same',\n",
        "input_shape=(128, 128, 3)))\n",
        "model.add(BatchNormalization())\n",
        "# Adds batchnorm before the activation\n",
        "model.add(ReLU())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096))\n",
        "model.add(ReLU())\n",
        "\n",
        "# Adds batchnorm after the activation\n",
        "model.add(BatchNormalization())\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPrSVZG3TgvA"
      },
      "source": [
        "##ResNet50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi8wDlg5Tkbx"
      },
      "source": [
        "ResNet50 is a well-known model, which is commonly reused as a stock model, such as\n",
        "for transfer learning, as shared layers in objection detection, and for performance benchmarking.\n",
        "\n",
        "ResNet50 v1 formalized the concept of a convolutional group. This is a set of convolutional\n",
        "blocks that share a common configuration, such as the number of filters. In v1,\n",
        "the neural network is decomposed into groups, and each group doubles the number\n",
        "of filters from the previous group.\n",
        "\n",
        "Additionally, the concept of a separate convolution block to double the number of\n",
        "filters was removed and replaced by a residual block that uses linear projection. Each\n",
        "group starts with a residual block using linear projection on the identity link to double\n",
        "the number of filters, while the remaining residual blocks pass the input directly to the output for the matrix add operation. Additionally, the first 1 × 1 convolution in\n",
        "the residual block with linear projection uses a stride of 2 (feature pooling), which is\n",
        "also known as a strided convolution, reducing the feature map sizes by 75%.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/3-convolutional-and-residual-neural-networks/images/5.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFlIv794UpvV"
      },
      "source": [
        "def identity_block(x, n_filters):\n",
        "  \"\"\"\n",
        "  Create a Bottleneck Residual Block of Convolutions\n",
        "  n_filters: number of filters\n",
        "  x        : input into the block\n",
        "  \"\"\"\n",
        "  shortcut = x\n",
        "\n",
        "  x = Conv2D(n_filters, kernel_size=(1, 1), strides=(1, 1))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  x = Conv2D(n_filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  \n",
        "  x = Conv2D(n_filters * 4, kernel_size=(1, 1), strides=(1, 1))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # Matrix addition of the input to the output\n",
        "  x = layers.add([shortcut, x])\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2cjR--dXK4F"
      },
      "source": [
        "def projection_block(x, n_filters, strides=(2, 2)):\n",
        "  \"\"\"\n",
        "  Create Block of Convolutions with feature pooling\n",
        "  Increase the number of filters by 4X\n",
        "  x        : input into the block\n",
        "  n_filters: number of filters\n",
        "  \"\"\"\n",
        "  # 1 × 1 projection convolution on shortcut to match size of output\n",
        "  shortcut = Conv2D(4 * n_filters, kernel_size=(1, 1), strides=strides)(x)\n",
        "  shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "  x = Conv2D(n_filters, kernel_size=(1, 1), strides=strides)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  x = Conv2D(n_filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  \n",
        "  x = Conv2D(4 * n_filters, kernel_size=(1, 1), strides=(1, 1))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # Matrix addition of the input to the output\n",
        "  x = layers.add([x, shortcut])\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbvtBv9UYRDA"
      },
      "source": [
        "# The input tensor  \n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "\n",
        "# First convolutional layer, where pooled feature maps will be reduced by 75%\n",
        "x = ZeroPadding2D(padding=(3, 3))(inputs)\n",
        "x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding=\"valid\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = ReLU()(x)\n",
        "x = ZeroPadding2D(padding=(1, 1))(x)\n",
        "x = MaxPool2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "\n",
        "# Each convolutional group after the first group starts with a projection block.\n",
        "x = projection_block(64, x, strides=(1, 1))\n",
        "\n",
        "# First identity block group of 64 filters\n",
        "for _ in range(2):\n",
        "  x = identity_block(64, x)\n",
        "x = projection_block(128, x)\n",
        "\n",
        "# Second identity block group of 128 filters\n",
        "for _ in range(3):\n",
        "  x = identity_block(128, x)\n",
        "x = projection_block(256, x)\n",
        "\n",
        "# Third identity block group of 256 filters\n",
        "for _ in range(5):\n",
        "  x = identity_block(256, x)\n",
        "x = projection_block(512, x)\n",
        "\n",
        "# Fourth identity block group of 512 filters\n",
        "for _ in range(2):\n",
        "  x = identity_block(512, x)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Output layer for classification (1000 classes)\n",
        "outputs = Dense(1000, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0U-X4JBizTz"
      },
      "source": [
        "v1.5 introduced a refactoring of the bottleneck design and\n",
        "further reducing of computational complexity, while maintaining representational\n",
        "power. The feature pooling (strides = 2) in the residual block with linear projection is\n",
        "moved from the first 1 × 1 convolution to the 3 × 3 convolution, reducing computational\n",
        "complexity and increasing results on ImageNet by 0.5%.\n",
        "\n",
        "<img src='https://github.com/rahiakela/computer-vision-research-and-practice/blob/main/deep-learning-patterns-and-practices/3-convolutional-and-residual-neural-networks/images/6.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbF8fQcDjhjV"
      },
      "source": [
        "def projection_block(x, n_filters, strides=(2, 2)):\n",
        "  \"\"\"\n",
        "  Create Block of Convolutions with feature pooling\n",
        "  Increase the number of filters by 4X\n",
        "  x        : input into the block\n",
        "  n_filters: number of filters\n",
        "  \"\"\"\n",
        "  # 1 × 1 projection convolution on shortcut to match size of output\n",
        "  shortcut = Conv2D(4 * n_filters, kernel_size=(1, 1), strides=strides)(x)\n",
        "  shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "  x = Conv2D(n_filters, kernel_size=(1, 1), strides=(1, 1))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  x = Conv2D(n_filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  \n",
        "  x = Conv2D(4 * n_filters, kernel_size=(1, 1), strides=(1, 1))(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # Matrix addition of the input to the output\n",
        "  x = layers.add([x, shortcut])\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQZTplvum1Qu"
      },
      "source": [
        "ResNet50 v2 introduced preactivation batch normalization (BN-RE-Conv), in which the batch normalization and activation functions are placed before (instead of after) the corresponding convolution or dense layer. \n",
        "\n",
        "This has now become a common practice, as depicted here for implementation of the residual block with the identity link in v2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8PK93y0m0ey"
      },
      "source": [
        "def identity_block(x, n_filters):\n",
        "  \"\"\"\n",
        "  Create a Bottleneck Residual Block of Convolutions\n",
        "  n_filters: number of filters\n",
        "  x        : input into the block\n",
        "  \"\"\"\n",
        "  shortcut = x\n",
        "\n",
        "  # Batchnorm before the convolution\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  x = Conv2D(n_filters, kernel_size=(1, 1), strides=(1, 1))(x)\n",
        "\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  x = Conv2D(n_filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
        "\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  x = Conv2D(n_filters * 4, kernel_size=(1, 1), strides=(1, 1))(x)\n",
        "\n",
        "  # Matrix addition of the input to the output\n",
        "  x = layers.add([shortcut, x])\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}