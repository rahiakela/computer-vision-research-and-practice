{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07-improving-results-with-tesseract-options.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMPaKaOW/inlVeUjfLHxZDF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/computer-vision-research-and-practice/blob/main/opencv-projects-and-guide/ocr-with-opencv-and-tesseract/07_improving_results_with_tesseract_options.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Improving OCR Results with Tesseract Options"
      ],
      "metadata": {
        "id": "Si_AnbHH8t4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "sudo apt install tesseract-ocr\n",
        "pip install tesseract\n",
        "pip install pytesseract\n",
        "pip install Pillow==9.0.0"
      ],
      "metadata": {
        "id": "CTdT0BiA8mNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just restart the colab environment."
      ],
      "metadata": {
        "id": "Dy5I4iPZASXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "4T18ensl8pDG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pytesseract.pytesseract.tesseract_cmd = (r'/usr/bin/tesseract')"
      ],
      "metadata": {
        "id": "6QMeK_4a-J6b"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download images."
      ],
      "metadata": {
        "id": "UR6-6OuaE9pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "wget https://github.com/rahiakela/computer-vision-research-and-practice/raw/main/opencv-projects-and-guide/ocr-with-opencv-and-tesseract/images/text-orient-1.png\n",
        "wget https://github.com/rahiakela/computer-vision-research-and-practice/raw/main/opencv-projects-and-guide/ocr-with-opencv-and-tesseract/images/text-orient-1.png"
      ],
      "metadata": {
        "id": "86MrGO6AFCFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tesseract --help-psm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_q7pgCjcEAj",
        "outputId": "8d51be3f-f307-48b1-ca3c-452a8c110c16"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page segmentation modes:\n",
            "  0    Orientation and script detection (OSD) only.\n",
            "  1    Automatic page segmentation with OSD.\n",
            "  2    Automatic page segmentation, but no OSD, or OCR.\n",
            "  3    Fully automatic page segmentation, but no OSD. (Default)\n",
            "  4    Assume a single column of text of variable sizes.\n",
            "  5    Assume a single uniform block of vertically aligned text.\n",
            "  6    Assume a single uniform block of text.\n",
            "  7    Treat the image as a single text line.\n",
            "  8    Treat the image as a single word.\n",
            "  9    Treat the image as a single word in a circle.\n",
            " 10    Treat the image as a single character.\n",
            " 11    Sparse text. Find as much text as possible in no particular order.\n",
            " 12    Sparse text with OSD.\n",
            " 13    Raw line. Treat the image as a single text line,\n",
            "       bypassing hacks that are Tesseract-specific.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PSM 0"
      ],
      "metadata": {
        "id": "WEqc2xwTFWjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Orientation and script detection (OSD) examines the input image, but instead of returning the\n",
        "actual OCRâ€™d text, OSD returns two values:\n",
        "\n",
        "* How the page is oriented, in degrees\n",
        "* The confidence of the script"
      ],
      "metadata": {
        "id": "LHVUg6zCUauM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_orientation(img_path, options):\n",
        "  image = cv2.imread(img_path)\n",
        "\n",
        "  image_bgr = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # determine the text orientation\n",
        "  results = pytesseract.image_to_osd(image_bgr, output_type=pytesseract.Output.DICT, config=options)\n",
        "\n",
        "  print(f\"Page number: {results['page_num']}\")\n",
        "  print(f\"Orientation: {results['orientation']}\")\n",
        "  print(f\"Rotate: {results['rotate']}\")\n",
        "  print(f\"Orientation confidence: {results['orientation_conf']}\")\n",
        "  print(f\"Script: {results['script']}\")\n",
        "  print(f\"Script confidence: {results['script_conf']}\")"
      ],
      "metadata": {
        "id": "WPbQHkMKhNL6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_orientation(\"text-orient-1.png\", options=\"--psm 0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WfLOFed9NIV",
        "outputId": "ba270b9e-81d1-4f8a-cda4-f5aba1ee8d41"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------\n",
            "Page number: 0\n",
            "Orientation: 0\n",
            "Rotate: 0\n",
            "Orientation confidence: 4.51\n",
            "Script: Latin\n",
            "Script confidence: 4.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_orientation(\"text-orient-2.png\", options=\"--psm 0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t-m5UZh_GaH",
        "outputId": "df1ca7a8-3bed-4314-d471-e1c7f5c63787"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page number: 0\n",
            "Orientation: 90\n",
            "Rotate: 270\n",
            "Orientation confidence: 3.7\n",
            "Script: Latin\n",
            "Script confidence: 8.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PSM 1"
      ],
      "metadata": {
        "id": "P6m2K9dMamiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def psm_options(img_path, options=None):\n",
        "  image = cv2.imread(img_path)\n",
        "\n",
        "  image_bgr = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # determine the text orientation\n",
        "  results = pytesseract.image_to_string(image_bgr, config=options)\n",
        "  return results"
      ],
      "metadata": {
        "id": "GxIC2bgla0pk"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = psm_options(\"text-orient-1.png\", options=\"--psm 1\")\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "iNP_YDBuAxiX",
        "outputId": "38fbc979-53f9-486c-8e7e-190543c96884"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In the first part of this tutorial, we'll discuss how\\nautoencoders can be used used for image retrieval\\nand building image search engines.\\n\\nFrom there, we'll implement a convolutional autoencoder\\nthat we'll then train on our image dataset.\\n\\x0c\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = psm_options(\"text-orient-2.png\", options=\"--psm 1\")\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "3mlWQsuyA2fj",
        "outputId": "72280464-8ab2-41e3-8edb-1cfb200e1794"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" \\n\\nIn the first part of this tutorial, we'll discuss how\\nautoencoders can be used used for image retrieval\\nand building image search engines.\\n\\nFrom there, we'll implement a convolutional autoencoder\\nthat we'll then train on our image dataset.\\n\\x0c\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PSM 3"
      ],
      "metadata": {
        "id": "3-nX5M04cJzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = psm_options(\"text-orient-1.png\", options=\"--psm 3\")\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "xzK1Bw-ucLF1",
        "outputId": "81ae4ef4-3ad4-42e5-fa81-15a53326e6b4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In the first part of this tutorial, we'll discuss how\\nautoencoders can be used used for image retrieval\\nand building image search engines.\\n\\nFrom there, we'll implement a convolutional autoencoder\\nthat we'll then train on our image dataset.\\n\\x0c\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = psm_options(\"text-orient-2.png\", options=\"--psm 3\")\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "I2CuUU-zcQBw",
        "outputId": "13419e85-a926-49f7-8673-ab6f17e03609"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" \\n\\nIn the first part of this tutorial, we'll discuss how\\nautoencoders can be used used for image retrieval\\nand building image search engines.\\n\\nFrom there, we'll implement a convolutional autoencoder\\nthat we'll then train on our image dataset.\\n\\x0c\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PSM 3 is the default behavior of Tesseract."
      ],
      "metadata": {
        "id": "htk_TDOgcbLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = psm_options(\"text-orient-1.png\")\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "SDqueZIucZ-c",
        "outputId": "0ea4f1ec-6fb5-4c68-bb4b-cf71916274cc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In the first part of this tutorial, we'll discuss how\\nautoencoders can be used used for image retrieval\\nand building image search engines.\\n\\nFrom there, we'll implement a convolutional autoencoder\\nthat we'll then train on our image dataset.\\n\\x0c\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = psm_options(\"text-orient-2.png\")\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "aOZjU9uAcj9e",
        "outputId": "53e652ca-e4d7-4f3b-edf2-a339409edfe4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" \\n\\nIn the first part of this tutorial, we'll discuss how\\nautoencoders can be used used for image retrieval\\nand building image search engines.\\n\\nFrom there, we'll implement a convolutional autoencoder\\nthat we'll then train on our image dataset.\\n\\x0c\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PSM 4"
      ],
      "metadata": {
        "id": "B_FD1yEydSfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5EQivC2rdUFZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}